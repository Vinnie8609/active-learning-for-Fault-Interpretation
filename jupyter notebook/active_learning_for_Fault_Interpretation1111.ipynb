{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8c0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up random seed\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f085fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "# from metrics import dice_coef\n",
    "# from metrics import dice\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def ConstraLoss(inputs, targets):\n",
    "\n",
    "    m=nn.AdaptiveAvgPool2d(1)\n",
    "    input_pro = m(inputs)\n",
    "    input_pro = input_pro.view(inputs.size(0),-1) #N*C\n",
    "    targets_pro = m(targets)\n",
    "    targets_pro = targets_pro.view(targets.size(0),-1)#N*C\n",
    "    input_normal = nn.functional.normalize(input_pro,p=2,dim=1) # 正则化\n",
    "    targets_normal = nn.functional.normalize(targets_pro,p=2,dim=1)\n",
    "    res = (input_normal - targets_normal)\n",
    "    res = res * res\n",
    "    loss = torch.mean(res)\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def dice_loss(score, target):\n",
    "    target = target.float()\n",
    "    smooth = 1e-5\n",
    "    intersect = torch.sum(score * target)\n",
    "    y_sum = torch.sum(target * target)\n",
    "    z_sum = torch.sum(score * score)\n",
    "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "    loss = 1 - loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dice_loss1(score, target):\n",
    "    target = target.float()\n",
    "    smooth = 1e-5\n",
    "    intersect = torch.sum(score * target)\n",
    "    y_sum = torch.sum(target)\n",
    "    z_sum = torch.sum(score)\n",
    "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "    loss = 1 - loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "def entropy_loss(p, C=2):\n",
    "    # p N*C*W*H*D\n",
    "    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1) / \\\n",
    "        torch.tensor(np.log(C)).cuda()\n",
    "    ent = torch.mean(y1)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "def softmax_dice_loss(input_logits, target_logits):\n",
    "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    input_softmax = F.softmax(input_logits, dim=1)\n",
    "    target_softmax = F.softmax(target_logits, dim=1)\n",
    "    n = input_logits.shape[1]\n",
    "    dice = 0\n",
    "    for i in range(0, n):\n",
    "        dice += dice_loss1(input_softmax[:, i], target_softmax[:, i])\n",
    "    mean_dice = dice / n\n",
    "\n",
    "    return mean_dice\n",
    "\n",
    "\n",
    "def entropy_loss_map(p, C=2):\n",
    "    ent = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
    "                       keepdim=True)/torch.tensor(np.log(C)).cuda()\n",
    "    return ent\n",
    "\n",
    "\n",
    "def softmax_mse_loss(input_logits, target_logits, sigmoid=False):\n",
    "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    if sigmoid:\n",
    "        input_softmax = torch.sigmoid(input_logits)\n",
    "        target_softmax = torch.sigmoid(target_logits)\n",
    "    else:\n",
    "        input_softmax = F.softmax(input_logits, dim=1)\n",
    "        target_softmax = F.softmax(target_logits, dim=1)\n",
    "\n",
    "    mse_loss = (input_softmax-target_softmax)**2\n",
    "    return mse_loss\n",
    "\n",
    "\n",
    "def softmax_kl_loss(input_logits, target_logits, sigmoid=False):\n",
    "    \"\"\"Takes softmax on both sides and returns KL divergence\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    if sigmoid:\n",
    "        input_log_softmax = torch.log(torch.sigmoid(input_logits))\n",
    "        target_softmax = torch.sigmoid(target_logits)\n",
    "    else:\n",
    "        input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
    "        target_softmax = F.softmax(target_logits, dim=1)\n",
    "\n",
    "    # return F.kl_div(input_log_softmax, target_softmax)\n",
    "    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='mean')\n",
    "    # mean_kl_div = torch.mean(0.2*kl_div[:,0,...]+0.8*kl_div[:,1,...])\n",
    "    return kl_div\n",
    "\n",
    "\n",
    "def symmetric_mse_loss(input1, input2):\n",
    "    \"\"\"Like F.mse_loss but sends gradients to both directions\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to both input1 and input2.\n",
    "    \"\"\"\n",
    "    assert input1.size() == input2.size()\n",
    "    return torch.mean((input1 - input2)**2)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1-alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            # N,C,H,W => N,C,H*W\n",
    "            input = input.view(input.size(0), input.size(1), -1)\n",
    "            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_classes):\n",
    "            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob)\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _dice_loss(self, score, target):\n",
    "        target = target.float()\n",
    "        smooth = 1e-5\n",
    "        intersect = torch.sum(score * target)\n",
    "        y_sum = torch.sum(target * target)\n",
    "        z_sum = torch.sum(score * score)\n",
    "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "##############加权\n",
    "    # def _dice_loss(self, score, target):\n",
    "    #     target = target.float()\n",
    "    #     smooth = 1e-5\n",
    "    #     alpha=1\n",
    "    #     target_with_facor=torch.sum(((1 - target) ** alpha) * target)\n",
    "        \n",
    "    #     intersect = torch.sum(score * target_with_facor)\n",
    "    #     y_sum = torch.sum(target_with_facor * target)\n",
    "    #     z_sum = torch.sum(score * score)\n",
    "    #     loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "    #     loss = 1 - loss\n",
    "    #     return loss\n",
    "\n",
    "\n",
    "    def forward(self, inputs, target, weight=None, softmax=False):\n",
    "        if softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "        target = self._one_hot_encoder(target)\n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_classes\n",
    "        assert inputs.size() == target.size(), 'predict & target shape do not match'\n",
    "        class_wise_dice = []\n",
    "        loss = 0.0\n",
    "        for i in range(0, self.n_classes):\n",
    "            dice = self._dice_loss(inputs[:, i], target[:, i])\n",
    "            class_wise_dice.append(1.0 - dice.item())\n",
    "            loss += dice * weight[i]\n",
    "        return loss / self.n_classes\n",
    "\n",
    "\n",
    "def entropy_minmization(p):\n",
    "    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1)\n",
    "    ent = torch.mean(y1)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "def entropy_map(p):\n",
    "    ent_map = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n",
    "                           keepdim=True)\n",
    "    return ent_map\n",
    "\n",
    "\n",
    "def compute_kl_loss(p, q):\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1),\n",
    "                      F.softmax(q, dim=-1), reduction='none')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1),\n",
    "                      F.softmax(p, dim=-1), reduction='none')\n",
    "\n",
    "    # Using function \"sum\" and \"mean\" are depending on your task\n",
    "    p_loss = p_loss.mean()\n",
    "    q_loss = q_loss.mean()\n",
    "\n",
    "    loss = (p_loss + q_loss) / 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "###############################################\n",
    "# BCE = torch.nn.BCELoss()\n",
    "\n",
    "def weighted_loss(pred, mask):\n",
    "    BCE = torch.nn.BCELoss(reduction = 'none')\n",
    "    \n",
    "    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask).float()\n",
    "    wbce = BCE(pred, mask)\n",
    "    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "    inter = ((pred * mask)*weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask)*weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1)/(union - inter+1)\n",
    "    \n",
    "    return (wbce + wiou).mean()  \n",
    "\n",
    "\n",
    "\n",
    "def calc_loss(pred, target, bce_weight=0.5):\n",
    "    bce = weighted_loss(pred, target)\n",
    "    # dl = 1 - dice_coef(pred, target)\n",
    "    # loss = bce * bce_weight + dl * bce_weight\n",
    "\n",
    "    return bce\n",
    "\n",
    "\n",
    "def loss_sup(logit_S1, logit_S2, labels_S1, labels_S2):\n",
    "    loss1 = calc_loss(logit_S1, labels_S1)\n",
    "    loss2 = calc_loss(logit_S2, labels_S2)\n",
    "\n",
    "    return loss1 + loss2\n",
    "\n",
    "\n",
    "\n",
    "def loss_diff(u_prediction_1, u_prediction_2, batch_size):\n",
    "    a = weighted_loss(u_prediction_1, Variable(u_prediction_2, requires_grad=False))\n",
    "#     print('a',a.size())\n",
    "    a = a.item()\n",
    "\n",
    "    b = weighted_loss(u_prediction_2, Variable(u_prediction_1, requires_grad=False))\n",
    "    b = b.item()\n",
    "\n",
    "    loss_diff_avg = (a + b)\n",
    "#     print('loss_diff_avg',loss_diff_avg)\n",
    "#     print('loss_diff batch size',batch_size)\n",
    "#     return loss_diff_avg / batch_size\n",
    "    return loss_diff_avg \n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "#contrastive_loss\n",
    "\n",
    "class ConLoss(torch.nn.Module):\n",
    "#for unlabel data\n",
    "    def __init__(self, temperature=0.07, base_temperature=0.07):\n",
    "        \"\"\"\n",
    "        Contrastive Learning for Unpaired Image-to-Image Translation\n",
    "        models/patchnce.py\n",
    "        \"\"\"\n",
    "        super(ConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.base_temperature = base_temperature\n",
    "        self.nce_includes_all_negatives_from_minibatch = False\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "#         self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction = 'none')\n",
    "        self.mask_dtype = torch.bool\n",
    "#         self.mask_dtype = torch.uint8 if version.parse(torch.__version__) < version.parse('1.2.0') else torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
    "        batch_size = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "#         width = feat_q.shape[2]\n",
    "        feat_q = feat_q.view(batch_size, dim, -1).permute(0, 2, 1)  #batch * dim * np  # batch * np * dim\n",
    "        feat_k = feat_k.view(batch_size, dim, -1).permute(0, 2, 1)\n",
    "        feat_q = F.normalize(feat_q, dim=-1, p=1)\n",
    "        feat_k = F.normalize(feat_k, dim=-1, p=1)\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.reshape(-1, 1, dim), feat_k.reshape(-1, dim, 1))  #(batch * np) * 1 * dim #(batch * np) * dim * 1  #(batch * np) * 1\n",
    "        l_pos = l_pos.view(-1, 1) #(batch * np) * 1\n",
    "\n",
    "        # neg logit\n",
    "        if self.nce_includes_all_negatives_from_minibatch:\n",
    "            # reshape features as if they are all negatives of minibatch of size 1.\n",
    "            batch_dim_for_bmm = 1\n",
    "        else:\n",
    "            batch_dim_for_bmm = batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.reshape(batch_dim_for_bmm, -1, dim)  #batch * np * dim\n",
    "        feat_k = feat_k.reshape(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))  # batch * np * np\n",
    "\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -float('inf'))\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)  #(batch * np) * np\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature  #(batch * np) * (np+1)\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class contrastive_loss_sup(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07, base_temperature=0.07):\n",
    "        \"\"\"\n",
    "        Contrastive Learning for Unpaired Image-to-Image Translation\n",
    "        models/patchnce.py\n",
    "        \"\"\"\n",
    "        super(contrastive_loss_sup, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.base_temperature = base_temperature\n",
    "        self.nce_includes_all_negatives_from_minibatch = False\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "#         self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction = 'none')\n",
    "        self.mask_dtype = torch.bool\n",
    "#         self.mask_dtype = torch.uint8 if version.parse(torch.__version__) < version.parse('1.2.0') else torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
    "        batch_size = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "#         width = feat_q.shape[2]\n",
    "        feat_q = feat_q.view(batch_size, dim, -1).permute(0, 2, 1)\n",
    "        feat_k = feat_k.view(batch_size, dim, -1).permute(0, 2, 1)\n",
    "        feat_q = F.normalize(feat_q, dim=-1, p=1)\n",
    "        feat_k = F.normalize(feat_k, dim=-1, p=1)\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "#         l_pos = torch.zeros((batch_size*2304,1)).cuda()\n",
    "#         l_pos = torch.zeros((batch_size*1024,1)).cuda()\n",
    "#         l_pos = torch.zeros((batch_size*784,1)).cuda()\n",
    "        # neg logit\n",
    "        if self.nce_includes_all_negatives_from_minibatch:\n",
    "            # reshape features as if they are all negatives of minibatch of size 1.\n",
    "            batch_dim_for_bmm = 1\n",
    "        else:\n",
    "            batch_dim_for_bmm = batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.reshape(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.reshape(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -float('inf'))\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "        l_pos = torch.zeros((l_neg.size(0),1)).cuda()\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def info_nce_loss(feats1,feats2):\n",
    "#     imgs, _ = batch\n",
    "#     imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "    # Encode all images\n",
    "#     feats = self.convnet(imgs)\n",
    "    # Calculate cosine similarity\n",
    "    cos_sim = F.cosine_similarity(feats1[:,None,:], feats2[None,:,:], dim=-1)\n",
    "    # Mask out cosine similarity to itself\n",
    "    self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "    cos_sim.masked_fill_(self_mask, -9e15)\n",
    "    # Find positive example -> batch_size//2 away from the original example\n",
    "    pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
    "    # InfoNCE loss\n",
    "    cos_sim = cos_sim / 0.07\n",
    "    nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "    nll = nll.mean()\n",
    "\n",
    "    # Logging loss\n",
    "#     self.log(mode+'_loss', nll)\n",
    "    # Get ranking position of positive example\n",
    "#     comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
    "#                               cos_sim.masked_fill(pos_mask, -9e15)],\n",
    "#                              dim=-1)\n",
    "#     sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "#     # Logging ranking metrics\n",
    "#     self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
    "#     self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
    "#     self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
    "\n",
    "    return nll\n",
    "\n",
    "class contrastive_loss_sup(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07, base_temperature=0.07):\n",
    "        \"\"\"\n",
    "        Contrastive Learning for Unpaired Image-to-Image Translation\n",
    "        models/patchnce.py\n",
    "        \"\"\"\n",
    "        super(contrastive_loss_sup, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.base_temperature = base_temperature\n",
    "        self.nce_includes_all_negatives_from_minibatch = False\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "#         self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction = 'none')\n",
    "        self.mask_dtype = torch.bool\n",
    "#         self.mask_dtype = torch.uint8 if version.parse(torch.__version__) < version.parse('1.2.0') else torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
    "        batch_size = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "#         width = feat_q.shape[2]\n",
    "        feat_q = feat_q.view(batch_size, dim, -1).permute(0, 2, 1)\n",
    "        feat_k = feat_k.view(batch_size, dim, -1).permute(0, 2, 1)\n",
    "        feat_q = F.normalize(feat_q, dim=-1, p=1)\n",
    "        feat_k = F.normalize(feat_k, dim=-1, p=1)\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.reshape(-1, 1, dim), feat_k.reshape(-1, dim, 1))  \n",
    "        l_pos = l_pos.view(-1, 1) \n",
    "        # neg logit\n",
    "        if self.nce_includes_all_negatives_from_minibatch:\n",
    "            # reshape features as if they are all negatives of minibatch of size 1.\n",
    "            batch_dim_for_bmm = 1\n",
    "        else:\n",
    "            batch_dim_for_bmm = batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.reshape(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.reshape(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -float('inf'))\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "#         l_pos = torch.zeros((l_neg.size(0),1)).cuda()\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "\n",
    "class MocoLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07, use_queue = True, max_queue = 1):\n",
    "\n",
    "        super(MocoLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.use_queue = use_queue\n",
    "        self.mask_dtype = torch.bool\n",
    "        self.queue = OrderedDict()\n",
    "        self.idx_list = []\n",
    "        self.max_queue = max_queue\n",
    "\n",
    "    def forward(self, feat_q, feat_k, idx):\n",
    "        num_enqueue = 0\n",
    "        num_update = 0\n",
    "        num_dequeue = 0\n",
    "        mid_pop = 0\n",
    "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
    "        dim = feat_q.shape[1]\n",
    "        batch_size = feat_q.shape[0]\n",
    "        feat_q = feat_q.reshape(batch_size,-1)  \n",
    "        feat_k = feat_k.reshape(batch_size,-1)\n",
    "\n",
    "        K = len(self.queue)\n",
    "#         print(K)\n",
    "\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = F.cosine_similarity(feat_q,feat_k,dim=1)        \n",
    "        l_pos = l_pos.view(-1, 1)\n",
    "\n",
    "        # neg logit\n",
    "        if K == 0 or not self.use_queue:\n",
    "            l_neg = F.cosine_similarity(feat_q[:,None,:], feat_k[None,:,:], dim=-1)\n",
    "        else:\n",
    "            for i in range(0,batch_size):\n",
    "                if str(idx[i].item()) in self.queue.keys():\n",
    "                    self.queue.pop(str(idx[i].item()))\n",
    "                    mid_pop += 1\n",
    "            queue_tensor = torch.cat(list(self.queue.values()),dim = 0)\n",
    "            l_neg = F.cosine_similarity(feat_q[:,None,:], queue_tensor.reshape(-1,feat_q.size(1))[None,:,:], dim=-1)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature  #batch_size * (K+1)\n",
    "        \n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "        \n",
    "        if self.use_queue:\n",
    "            for i in range(0,batch_size):\n",
    "                if str(idx[i].item()) not in self.queue.keys():\n",
    "                    self.queue[str(idx[i].item())] = feat_k[i].clone()[None,:]\n",
    "                    num_enqueue += 1\n",
    "                else:\n",
    "                    self.queue[str(idx[i].item())] = feat_k[i].clone()[None,:]\n",
    "                    num_update += 1\n",
    "                if len(self.queue) >= 1056 + 1:\n",
    "                    self.queue.popitem(False)\n",
    "\n",
    "                    num_dequeue += 1\n",
    "\n",
    "#         print('queue length, mid pop, enqueue, update queue, dequeue: ', len(self.queue), mid_pop, num_enqueue, num_update, num_dequeue)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class ConLoss_queue(torch.nn.Module):\n",
    "#for unlabel data\n",
    "    def __init__(self, temperature=0.07, use_queue = True, max_queue = 1):\n",
    "        \"\"\"\n",
    "        Contrastive Learning for Unpaired Image-to-Image Translation\n",
    "        models/patchnce.py\n",
    "        \"\"\"\n",
    "        super(ConLoss_queue, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.base_temperature = base_temperature\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.mask_dtype = torch.bool\n",
    "        self.queue = OrderedDict()\n",
    "        self.idx_list = []\n",
    "        self.max_queue = max_queue\n",
    "\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        num_enqueue = 0\n",
    "        num_update = 0\n",
    "        num_dequeue = 0\n",
    "        mid_pop = 0\n",
    "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
    "        batch_size = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "#         width = feat_q.shape[2]\n",
    "        feat_q = feat_q.view(batch_size, dim, -1).permute(0, 2, 1)  #batch * dim * np  # batch * np * dim\n",
    "        feat_k = feat_k.view(batch_size, dim, -1).permute(0, 2, 1)\n",
    "        feat_q = F.normalize(feat_q, dim=-1, p=1)\n",
    "        feat_k = F.normalize(feat_k, dim=-1, p=1)\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.reshape(-1, 1, dim), feat_k.reshape(-1, dim, 1))  #(batch * np) * 1 * dim #(batch * np) * dim * 1  #(batch * np) * 1\n",
    "        l_pos = l_pos.view(-1, 1) #(batch * np) * 1\n",
    "\n",
    "        # neg logit\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.reshape(batch_size, -1, dim)  #batch * np * dim\n",
    "        feat_k = feat_k.reshape(batch_size, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))  # batch * np * np\n",
    "\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -float('inf'))\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)  #(batch * np) * np\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature  #(batch * np) * (np+1)\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "class MocoLoss_list(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07, use_queue = True):\n",
    "\n",
    "        super(MocoLoss_list, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.use_queue = use_queue\n",
    "        self.queue = []\n",
    "        self.mask_dtype = torch.bool\n",
    "        self.idx_list = []\n",
    "\n",
    "    def forward(self, feat_q, feat_k, idx):\n",
    "        assert feat_q.size() == feat_k.size(), (feat_q.size(), feat_k.size())\n",
    "        dim = feat_q.shape[1]\n",
    "        batch_size = feat_q.shape[0]\n",
    "        feat_q = feat_q.reshape(batch_size,-1)  #转成向量\n",
    "        feat_k = feat_k.reshape(batch_size,-1)\n",
    "\n",
    "        K = len(self.queue)\n",
    "#         print('K',K)\n",
    "\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = F.cosine_similarity(feat_q,feat_k,dim=1)        \n",
    "        l_pos = l_pos.view(-1, 1)\n",
    "\n",
    "        # neg logit\n",
    "        if K == 0 or not self.use_queue:\n",
    "            l_neg = F.cosine_similarity(feat_q[:,None,:], feat_k[None,:,:], dim=-1)\n",
    "        else:            \n",
    "            queue_tensor = torch.cat(self.queue,dim = 0)\n",
    "            print(queue_tensor.size())\n",
    "            l_neg = F.cosine_similarity(feat_q[:,None,:], queue_tensor.reshape(-1,feat_q.size(1))[None,:,:], dim=-1)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.temperature  #batch_size * (K+1)\n",
    "        \n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "        if self.use_queue:\n",
    "            self.queue.append(feat_k.clone())\n",
    "#             for i in range(0,24):\n",
    "#                 if idx[i] not in self.idx_list and len(self.queue) <512:\n",
    "# #                     print(idx[i].item())\n",
    "# #                     print(self.idx_list)\n",
    "#                     self.idx_list.append(idx[i].item())                    \n",
    "#                     self.queue.append(feat_k[i].clone()[None,:])\n",
    "#                     print('LIST',len(self.idx_list))\n",
    "#                     print('1',feat_k[i][None,:].size())\n",
    "#                 elif idx[i] in self.idx_list:\n",
    "#                     print('duplicate')\n",
    "            if K >= 512:\n",
    "#                 print('pop')\n",
    "                self.queue.pop(0)\n",
    "#                 self.idx_list.pop(0)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac87f19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86177\\.conda\\envs\\mypytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\86177\\.conda\\envs\\mypytorch\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "#image_tool\n",
    "import torch\n",
    "import cv2\n",
    "from albumentations import *\n",
    "import numpy as np\n",
    "from itertools import compress, product\n",
    "from skimage.util.shape import view_as_windows\n",
    "from typing import Tuple\n",
    "import scipy.signal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def norm(original):# normalize to [0,1]\n",
    "    d_min=original.min()\n",
    "    if d_min<0:\n",
    "        original+=torch.abs(d_min)\n",
    "        d_min=original.min()\n",
    "    d_max=original.max()\n",
    "    dst=d_max-d_min\n",
    "    norm_data=(original-d_min).true_divide(dst)\n",
    "    return norm_data\n",
    "\n",
    "\n",
    "def augument(p=0.5):\n",
    "    return Compose([\n",
    "    OneOf([\n",
    "        HorizontalFlip(p=p),\n",
    "        VerticalFlip(p=p),\n",
    "    ]),\n",
    "    #1\n",
    "    # OneOf([\n",
    "    #     Sharpen(p=p),\n",
    "    #     # Emboss(p=1),\n",
    "    #     Blur(p=p)\n",
    "    # ], p=p),\n",
    "        #2\n",
    "    # ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30,interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
    "\n",
    "    ])\n",
    "def faultseg_augumentation(p=1):\n",
    "    return Compose([\n",
    "    # OneOf([\n",
    "    #     HorizontalFlip(p=p),\n",
    "    #     VerticalFlip(p=p),\n",
    "    #     Compose([VerticalFlip(p=p), HorizontalFlip(p=p)]),\n",
    "    # ]),######################################\n",
    "    #1\n",
    "    OneOf([\n",
    "        Sharpen(p=p),\n",
    "        # Emboss(p=1),\n",
    "        Blur(p=p)\n",
    "    ], p=p),\n",
    "        #2\n",
    "    # ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30,interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
    "   # 3\n",
    "    OneOf([\n",
    "        # RandomBrightnessContrast(p=1),\n",
    "        ElasticTransform(p=p, alpha=400, sigma=400 * 0.05, alpha_affine=400 * 0.03),\n",
    "        GridDistortion(p=p),\n",
    "        OpticalDistortion(p=p)\n",
    "    ],p=p)\n",
    "    ])\n",
    "def strongaug(seismic,fault):\n",
    "\n",
    "    # array = np.random.randint(0,2,5)\n",
    "\n",
    "    aug = VerticalFlip(p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "    aug = HorizontalFlip(p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    aug = Compose([VerticalFlip(p=1), HorizontalFlip(p=1)])\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    aug = ElasticTransform(p=1, alpha=400, sigma=400 * 0.05, alpha_affine=400 * 0.03)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    aug = GridDistortion(p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    Sharpen(p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    Emboss(p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    RandomBrightnessContrast(p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "\n",
    "    OpticalDistortion(p=1)\n",
    "    augmented = aug(image=seismic, mask=fault)\n",
    "    seismic, fault = augmented['image'], augmented['mask']\n",
    "    return seismic, fault\n",
    "\n",
    "\n",
    "def crop2(variable, th, tw):  # this is for crop center when outputs are 96*96\n",
    "    h, w = variable.shape[-2], variable.shape[-1]\n",
    "    x1 = int(round((w - tw) / 2.))\n",
    "    y1 = int(round((h - th) / 2.))\n",
    "    return variable[:, :, y1: y1 + th, x1: x1 + tw]\n",
    "\n",
    "cached_2d_windows = dict()\n",
    "\n",
    "\n",
    "\n",
    "def window_2D(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Make a 1D window function, then infer and return a 2D window function.\n",
    "    Done with an augmentation, and self multiplication with its transpose.\n",
    "    Could be generalized to more dimensions.\n",
    "    \"\"\"\n",
    "    # Memoization\n",
    "    global cached_2d_windows\n",
    "    key = \"{}_{}\".format(window_size, power)\n",
    "    if key in cached_2d_windows:\n",
    "        wind = cached_2d_windows[key]\n",
    "    else:\n",
    "        wind = spline_window(window_size, power)\n",
    "        wind = np.expand_dims(np.expand_dims(wind, -1), -1)\n",
    "        wind = wind * wind.transpose(1, 0, 2)\n",
    "        cached_2d_windows[key] = wind\n",
    "    return wind\n",
    "\n",
    "def spline_window(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Squared spline (power=2) window function:\n",
    "    https://www.wolframalpha.com/input/?i=y%3Dx**2,+y%3D-(x-2)**2+%2B2,+y%3D(x-4)**2,+from+y+%3D+0+to+2\n",
    "    \"\"\"\n",
    "    intersection = int(window_size / 4)\n",
    "    wind_outer = (abs(2 * (scipy.signal.triang(window_size))) ** power) / 2\n",
    "    wind_outer[intersection:-intersection] = 0\n",
    "\n",
    "    wind_inner = 1 - (abs(2 * (scipy.signal.triang(window_size) - 1)) ** power) / 2\n",
    "    wind_inner[:intersection] = 0\n",
    "    wind_inner[-intersection:] = 0\n",
    "\n",
    "    wind = wind_inner + wind_outer\n",
    "    wind = wind / np.average(wind)\n",
    "    return wind\n",
    "\n",
    "\n",
    "def split_Image(bigImage, isMask, top_pad, bottom_pad, left_pad, right_pad, splitsize, stepsize, vertical_splits_number,\n",
    "                horizontal_splits_number):\n",
    "    #     print(bigImage.shape)\n",
    "    if isMask == True:\n",
    "        arr = np.pad(bigImage, ((top_pad, bottom_pad), (left_pad, right_pad)), \"reflect\")\n",
    "        splits = view_as_windows(arr, (splitsize, splitsize), step=stepsize)#(66, 270, 58, 58)\n",
    "        splits = splits.reshape((vertical_splits_number * horizontal_splits_number, splitsize, splitsize))\n",
    "    else:\n",
    "        arr = np.pad(bigImage, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), \"reflect\")\n",
    "        splits = view_as_windows(arr, (splitsize, splitsize, 3), step=stepsize)\n",
    "        splits = splits.reshape((vertical_splits_number * horizontal_splits_number, splitsize, splitsize, 3))\n",
    "    return splits  # return list of arrays.\n",
    "\n",
    "\n",
    "# idea from https://github.com/dovahcrow/patchify.py\n",
    "def recover_Image(patches: np.ndarray, imsize: Tuple[int, int, int], left_pad, right_pad, top_pad, bottom_pad,\n",
    "                  overlapsize):\n",
    "    #     patches = np.squeeze(patches)\n",
    "    assert len(patches.shape) == 5\n",
    "\n",
    "    i_h, i_w, i_chan = imsize\n",
    "    image = np.zeros((i_h + top_pad + bottom_pad, i_w + left_pad + right_pad, i_chan), dtype=patches.dtype)\n",
    "    divisor = np.zeros((i_h + top_pad + bottom_pad, i_w + left_pad + right_pad, i_chan), dtype=patches.dtype)\n",
    "\n",
    "    #     print(\"i_h, i_w, i_chan\",i_h, i_w, i_chan)\n",
    "    n_h, n_w, p_h, p_w, _ = patches.shape\n",
    "\n",
    "    o_w = overlapsize\n",
    "    o_h = overlapsize\n",
    "\n",
    "    s_w = p_w - o_w\n",
    "    s_h = p_h - o_h\n",
    "\n",
    "    for i, j in product(range(n_h), range(n_w)):\n",
    "        patch = patches[i, j]\n",
    "        image[(i * s_h):(i * s_h) + p_h, (j * s_w):(j * s_w) + p_w] += patch\n",
    "        divisor[(i * s_h):(i * s_h) + p_h, (j * s_w):(j * s_w) + p_w] += 1\n",
    "\n",
    "    recover = image / divisor\n",
    "    return recover[top_pad:top_pad + i_h, left_pad:left_pad + i_w]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Header\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class FAULTSEG_Handler(Dataset):\n",
    "    def __init__(self, X, Y,isTrain):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isTrain=isTrain\n",
    "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "\n",
    "    def transform(self, img, mask):\n",
    "        # to tensor\n",
    "        img = TF.to_tensor(img)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        img=norm(img)\n",
    "        img = TF.normalize(img, [2.69254e-05,],[0.1701577, ])############## [0.4915, ], [0.0655, ], mean=0.5   std=0.5\n",
    "        return img, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.X[index], self.Y[index]\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        y= np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if self.isTrain:  # 训练集，数据增强\n",
    "            aug = faultseg_augumentation(p=0.7)\n",
    "\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x = augmented['image']\n",
    "            y = augmented['mask']\n",
    "\n",
    "\n",
    "        # x = Image.fromarray(x.numpy(), mode='L')\n",
    "        x,y=self.transform(x,y)\n",
    "        return x, y, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "   \n",
    "class THEBE_Handler(Dataset):\n",
    "    def __init__(self, X, Y,isTrain):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isTrain=isTrain\n",
    "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "\n",
    "    def transform(self, img, mask):\n",
    "        # to tensor\n",
    "        img = TF.to_tensor(img)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        img=norm(img)\n",
    "        img = TF.normalize(img, [-3.26645e-05, ],[0.03790, ])############## [0.4915, ], [0.0655, ]      [0.000384, ],[1.05163, ]\n",
    "        return img, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        x, y = self.X[index], self.Y[index]\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        y= np.asarray(y, dtype=np.float32)\n",
    "\n",
    "        if self.isTrain:  # 训练集，数据增强\n",
    "            aug = faultseg_augumentation(p=0.7)\n",
    "\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x = augmented['image']\n",
    "            y = augmented['mask']\n",
    "\n",
    "\n",
    "        # x = Image.fromarray(x.numpy(), mode='L')\n",
    "        x,y=self.transform(x,y)\n",
    "        return x, y, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9658ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, X_train_first, Y_train_first,X_train_middle, Y_train_middle,X_train_small, Y_train_small, X_val, Y_val,X_test, Y_test, handler):\n",
    "        self.X_train_first = X_train_first\n",
    "        self.Y_train_first = Y_train_first\n",
    "        self.X_train_middle = X_train_middle\n",
    "        self.Y_train_middle = Y_train_middle\n",
    "        self.X_train_small = X_train_small\n",
    "        self.Y_train_small = Y_train_small\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.handler = handler\n",
    "        \n",
    "        self.n_pool = len(X_train_first)   #4000   100\n",
    "        self.n_test = len(X_test)   #1000\n",
    "        \n",
    "        self.labeled_idxs = np.zeros(self.n_pool, dtype=bool)   #4000 \n",
    "        \n",
    "    def initialize_labels(self, num):   #num=1000\n",
    "        # generate initial labeled pool\n",
    "        tmp_idxs = np.arange(self.n_pool)\n",
    "        np.random.shuffle(tmp_idxs)\n",
    "        self.labeled_idxs[tmp_idxs[:num]] = True\n",
    "        # print(self.labeled_idxs[tmp_idxs[:num]])\n",
    "    \n",
    "    def get_labeled_data(self):\n",
    "        # labeled_idxs = np.arange(self.n_pool)[self.labeled_idxs]\n",
    "        # print(\"aaaa\",self.labeled_idxs)\n",
    "        train_img=self.X_train_first\n",
    "        print(train_img.shape)\n",
    "        print(self.X_train_small.shape)\n",
    "        train_mask=self.Y_train_first\n",
    "        train_imgs_small=np.load(\"./data/THEBE224/val_img.npy\")\n",
    "        train_imgs_small=torch.tensor(train_imgs_small)\n",
    "        \n",
    "        # train_masks_small=np.load(\"/home/user/data/liuyue/active_learning/data/THEBE_NEW/train_mask_small.npy\")\n",
    "        train_masks_small=np.load(\"./data/THEBE224/val_img.npy\")\n",
    "        train_masks_small=torch.tensor(train_masks_small)\n",
    "        print(train_imgs_small.shape)\n",
    "        # if torch.sum(train_imgs_small)==0:\n",
    "        new_x_train=train_img\n",
    "        new_y_train=train_mask\n",
    "        # else:\n",
    "        #     new_x_train=torch.cat((train_img, train_imgs_small), dim=0)\n",
    "        #     new_y_train=torch.cat((train_mask, train_masks_small), dim=0)\n",
    "        return  self.handler(new_x_train, new_y_train,True)\n",
    "    \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "    def get_unlabeled_data(self):\n",
    "        unlabeled_idxs = np.arange(1)\n",
    "        return unlabeled_idxs, self.handler(self.X_train_middle, self.Y_train_middle,True)\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        # print(\"aaaa\",self.labeled_idxs)\n",
    "        return self.labeled_idxs.copy(), self.handler(self.X_train, self.Y_train,True)\n",
    "    \n",
    "    def get_val_data(self):\n",
    "        return self.handler(self.X_val, self.Y_val,False)\n",
    "        \n",
    "    def get_test_data(self):\n",
    "        return self.handler(self.X_test, self.Y_test,False)\n",
    "    \n",
    "    \n",
    "    def cal_test_acc(self, preds):\n",
    "        # return 1.0 * (self.Y_test==preds).sum().item() / self.n_test\n",
    "       pass\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_THEBE(handler):\n",
    "\n",
    "    img=np.load(\"./data/seistrain.npy\")\n",
    "    mask=np.load(\"./data/faulttrain.npy\")\n",
    "    num_frames = img.shape[0]\n",
    "    selected_indices = np.random.choice(num_frames, size=25, replace=False)\n",
    "    selected_images = img[selected_indices]\n",
    "    selected_masks = mask[selected_indices]\n",
    "    label_img= selected_images [:10,100:2148,700:1212]\n",
    "    unlabel_img= selected_images [10:,100:2148,700:1212]\n",
    "    label_mask= selected_masks [:10,100:2148,700:1212]\n",
    "    unlabel_mask= selected_masks [10:,100:2148,700:1212]\n",
    "    \n",
    "    trainimg=np.zeros([180,224,224])\n",
    "    trainmask=np.zeros([180,224,224])\n",
    "    id=0\n",
    "    for k in range(10):\n",
    "        for i in range (9):\n",
    "            for j in range(2):\n",
    "                img1=label_img[k,i*224:i*224+224,j*224:j*224+224]\n",
    "                mask1=label_mask[k,i*224:i*224+224,j*224:j*224+224]\n",
    "                if mask1.sum!=0:\n",
    "                    trainimg[id]=img1\n",
    "                    trainmask[id]=mask1\n",
    "                    id+=1\n",
    "    trainimg=trainimg[:id]\n",
    "    trainmask=trainmask[:id]\n",
    "\n",
    "    np.save(\"./data/trainimg.npy\",trainimg)\n",
    "    np.save(\"./data/trainmask.npy\",trainmask)\n",
    "    np.save(\"./data/trainimg_unlabel\",unlabel_img)\n",
    "    np.save(\"./data/trainmask_unlabel\",unlabel_mask)\n",
    "    \n",
    "    train_imgs_first=np.load(\"./data/trainimg.npy\") # 348*128*128\n",
    "   \n",
    "    train_imgs_first=torch.tensor(train_imgs_first)\n",
    "    \n",
    "    train_masks_first=np.load(\"./data/trainmask.npy\")\n",
    "   \n",
    "    train_masks_first=torch.tensor(train_masks_first)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_imgs_middle=np.load(\"./data/trainimg_unlabel.npy\")\n",
    "    train_imgs_middle=torch.tensor(train_imgs_middle)\n",
    "    \n",
    "    \n",
    "    train_masks_middle=np.load(\"./data/trainmask_unlabel.npy\")  \n",
    "    train_masks_middle=torch.tensor(train_masks_middle)\n",
    "\n",
    "    trainimg_small=np.zeros([50,224,224])\n",
    "    trainmask_small=np.zeros([50,224,224])\n",
    "    np.save(\"./data/trainimg_small.npy\",trainimg_small)\n",
    "    np.save(\"./data/trainmask_small.npy\",trainmask_small)\n",
    "   \n",
    "    train_imgs_small=np.load(\"./data/trainimg_small.npy\")\n",
    "    train_imgs_small=torch.tensor(train_imgs_small)\n",
    "\n",
    "    train_masks_small=np.load(\"./data//trainmask_small.npy\")\n",
    "    train_masks_small=torch.tensor(train_masks_small)\n",
    "\n",
    "    img=np.load(\"./data/seisval.npy\")\n",
    "    mask=np.load(\"./data/faultval.npy\")\n",
    "    num_frames = img.shape[0]\n",
    "    selected_indices = np.random.choice(num_frames, size=10, replace=False)\n",
    "    selected_images = img[selected_indices]\n",
    "    selected_masks = mask[selected_indices]\n",
    "    label_img= selected_images [:,100:2148,700:1212]\n",
    "    label_mask= selected_masks [:,100:2148,700:1212]\n",
    "    \n",
    "    \n",
    "    valimg=np.zeros([180,224,224])\n",
    "    valmask=np.zeros([180,224,224])\n",
    "    id=0\n",
    "    for k in range(10):\n",
    "        for i in range (9):\n",
    "            for j in range(2):\n",
    "                img1=label_img[k,i*224:i*224+224,j*224:j*224+224]\n",
    "                mask1=label_mask[k,i*224:i*224+224,j*224:j*224+224]\n",
    "                if mask1.sum!=0:\n",
    "                    valimg[id]=img1\n",
    "                    valmask[id]=mask1\n",
    "                    id+=1\n",
    "    valimg=valimg[:id]\n",
    "    valmask=valmask[:id]\n",
    "\n",
    "    np.save(\"./data/valimg.npy\",valimg)\n",
    "    np.save(\"./data/valmask.npy\",valmask)\n",
    "    \n",
    "    val_imgs=np.load(\"./data/valimg.npy\")\n",
    "    \n",
    "    val_imgs=torch.tensor(val_imgs)\n",
    "\n",
    "    val_masks=np.load(\"./data/valmask.npy\")\n",
    "    \n",
    "    val_masks=torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "    \n",
    "    test_imgs=np.load(\"./data/seistest.npy\")\n",
    "    \n",
    "    test_imgs=torch.tensor(test_imgs)\n",
    "\n",
    "   \n",
    "    test_masks=np.load(\"./data/faulttest.npy\")\n",
    "  \n",
    "    test_masks=torch.tensor(test_masks)\n",
    "  \n",
    "\n",
    "    return Data(train_imgs_first,train_masks_first, train_imgs_middle,train_masks_middle,train_imgs_small,train_masks_small, val_imgs, val_masks, test_imgs, test_masks, handler)\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "def get_FAULTSEG(handler):\n",
    "    train_imgs=np.load(\"./data/faultseg/train/seis/train_img.npy\")\n",
    "\n",
    "    train_imgs=torch.tensor(train_imgs)\n",
    "    \n",
    "    train_masks=np.load(\"./data/faultseg/train/fault/train_mask.npy\")\n",
    "      \n",
    "    train_masks=torch.tensor(train_masks)\n",
    "\n",
    "\n",
    "    val_imgs=np.load(\"./data/faultseg/validation/seis/val_img.npy\")\n",
    "     \n",
    "    val_imgs=torch.tensor(val_imgs)\n",
    "\n",
    "    val_masks=np.load(\"./data/faultseg/validation/fault/val_mask.npy\")\n",
    "    \n",
    "    val_masks=torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "\n",
    "    test_imgs=np.load(\"./data/THEBE/test_imgs.npy\")\n",
    "     \n",
    "    test_imgs=torch.tensor(test_imgs)\n",
    "\n",
    "    test_masks=np.load(\"./data/THEBE/test_masks.npy\")\n",
    "    \n",
    "    test_masks=torch.tensor(test_masks)\n",
    "  \n",
    "\n",
    "\n",
    "    return Data(train_imgs[:1000],train_masks[:1000], val_imgs[:40], val_masks[:40], test_imgs[:20], test_masks[:20], handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d257889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n"
     ]
    }
   ],
   "source": [
    "#common_tools\n",
    "import logging\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "from torch import nn\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "def resize(img,size,fill=0,method='padding'):\n",
    "    _, ow, oh = img.shape\n",
    "    diff_x = size - ow\n",
    "    diff_y = size - oh\n",
    "    if method=='constant_padding':\n",
    "        img = F.pad(img, [diff_x // 2, diff_x - diff_x // 2,\n",
    "                        diff_y // 2, diff_y - diff_y // 2], 'constant',fill)\n",
    "    elif method=='reflect_padding':\n",
    "        pad=nn.ReflectionPad2d([diff_x // 2, diff_x - diff_x // 2,\n",
    "                        diff_y // 2, diff_y - diff_y // 2])\n",
    "        img=pad(img)\n",
    "    elif method=='replication_padding':\n",
    "        pad=nn.ReplicationPad2d([diff_x // 2, diff_x - diff_x // 2,\n",
    "                        diff_y // 2, diff_y - diff_y // 2])\n",
    "        img=pad(img)\n",
    "    elif method==\"interpolate\":\n",
    "        CROP_SIZE=_pair(size)\n",
    "        img=img.unsqueeze(0)\n",
    "        # linear | bilinear | bicubic | trilinear\n",
    "        img = F.interpolate(img, size=CROP_SIZE, mode='bicubic', align_corners=False)\n",
    "        img=img.squeeze(0)\n",
    "    elif method=='extend':\n",
    "        img=img.squeeze(0)\n",
    "        left_img=img[:diff_x // 2]\n",
    "        right=diff_x - diff_x // 2\n",
    "        right_img=img[-right:]\n",
    "        print(left_img.shape,right_img.shape)\n",
    "        img=torch.concat([left_img,img,right_img],dim=0)\n",
    "        up_img = img[:, :diff_y // 2]\n",
    "        down = diff_y - diff_y // 2\n",
    "        down_img = img[:, -down:]\n",
    "        print(\"111  \",up_img.shape,img.shape,down_img.shape)\n",
    "        img=torch.concat([up_img,img,down_img],dim=1)\n",
    "        img=img.unsqueeze(0)\n",
    "        print(img.shape)\n",
    "    elif method=='hybrid':\n",
    "        # step1:插值到180\n",
    "        _,w,h=img.shape\n",
    "        interplot_extendsize=(size-w)//3+w\n",
    "        CROP_SIZE = _pair(interplot_extendsize)\n",
    "        img = img.unsqueeze(0)\n",
    "        # linear | bilinear | bicubic | trilinear\n",
    "        img = F.interpolate(img, size=CROP_SIZE, mode='bicubic', align_corners=False)\n",
    "        img = img.squeeze(0)\n",
    "        # step2:replication\n",
    "        pad = nn.ReplicationPad2d([diff_x // 2, diff_x - diff_x // 2,\n",
    "                                   0, 0])\n",
    "        img = pad(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def getPartDatasets(list,rate,seed=1234):\n",
    "\n",
    "    count=len(list)\n",
    "    train_num=int(count*rate)\n",
    "    train_list=[]\n",
    "    setup_seed(seed)\n",
    "    train_idx = random.sample(range(0, count),train_num)\n",
    "    for item in train_idx:\n",
    "        train_list.append(list[item])\n",
    "    return train_list\n",
    "\n",
    "\n",
    "def acc_metrics(outputs, labels):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(outputs)):\n",
    "    # TP    predict 和 label 同时为1\n",
    "        TP += ((outputs[i] == 1) & (labels[i] == 1)).sum()\n",
    "        # TN    predict 和 label 同时为0\n",
    "        TN += ((outputs[i] == 0) & (labels[i] == 0)).sum()\n",
    "        # FN    predict 0 label 1\n",
    "        FN += ((outputs[i] == 0) & (labels[i] == 1)).sum()\n",
    "        # FP    predict 1 label 0\n",
    "        FP += ((outputs[i] == 1) & (labels[i] == 0)).sum()\n",
    "    p = TP / (TP + FP)\n",
    "    r = TP / (TP + FN)\n",
    "    F1 = 2 * r * p / (r + p)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    return p, r, F1, acc\n",
    "\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor,smooth):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "\n",
    "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "\n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum((1, 2))  # Will be zzero if both are 0\n",
    "    iou = (intersection + smooth) / (union + smooth)  # We smooth our devision to avoid 0/0\n",
    "    return iou\n",
    "\n",
    "\n",
    "def setup_seed(seed=12345):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)     # cpu\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True       # 训练集变化不大时使训练加速，是固定cudnn最优配置，如卷积算法\n",
    "\n",
    "\n",
    "def show_confMat(confusion_mat, classes, set_name, out_dir, epoch=999, verbose=False, perc=False):\n",
    "    \"\"\"\n",
    "    混淆矩阵绘制并保存图片\n",
    "    :param confusion_mat:  nd.array\n",
    "    :param classes: list or tuple, 类别名称\n",
    "    :param set_name: str, 数据集名称 train or valid or test_from_anyu?\n",
    "    :param out_dir:  str, 图片要保存的文件夹\n",
    "    :param epoch:  int, 第几个epoch\n",
    "    :param verbose: bool, 是否打印精度信息\n",
    "    :param perc: bool, 是否采用百分比，图像分割时用，因分类数目过大\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cls_num = len(classes)\n",
    "\n",
    "    # 归一化\n",
    "    confusion_mat_tmp = confusion_mat.copy()\n",
    "    for i in range(len(classes)):\n",
    "        confusion_mat_tmp[i, :] = confusion_mat[i, :] / confusion_mat[i, :].sum()\n",
    "\n",
    "    # 设置图像大小\n",
    "    if cls_num < 10:\n",
    "        figsize = 6\n",
    "    elif cls_num >= 100:\n",
    "        figsize = 30\n",
    "    else:\n",
    "        figsize = np.linspace(6, 30, 91)[cls_num-10]\n",
    "    plt.figure(figsize=(int(figsize), int(figsize*1.3)))\n",
    "\n",
    "    # 获取颜色\n",
    "    cmap = plt.cm.get_cmap('Greys')  # 更多颜色: http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "    plt.imshow(confusion_mat_tmp, cmap=cmap)\n",
    "    plt.colorbar(fraction=0.03)\n",
    "\n",
    "    # 设置文字\n",
    "    xlocations = np.array(range(len(classes)))\n",
    "    plt.xticks(xlocations, list(classes), rotation=60)\n",
    "    plt.yticks(xlocations, list(classes))\n",
    "    plt.xlabel('Predict label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title(\"Confusion_Matrix_{}_{}\".format(set_name, epoch))\n",
    "\n",
    "    # 打印数字\n",
    "    if perc:\n",
    "\n",
    "        cls_per_nums = confusion_mat.sum(axis=1).reshape((cls_num, 1))\n",
    "        conf_mat_per = confusion_mat / cls_per_nums\n",
    "        for i in range(confusion_mat_tmp.shape[0]):\n",
    "            for j in range(confusion_mat_tmp.shape[1]):\n",
    "                plt.text(x=j, y=i, s=\"{:.0%}\".format(conf_mat_per[i, j]), va='center', ha='center', color='red',\n",
    "                         fontsize=10)\n",
    "    else:\n",
    "        for i in range(confusion_mat_tmp.shape[0]):\n",
    "            for j in range(confusion_mat_tmp.shape[1]):\n",
    "                plt.text(x=j, y=i, s=int(confusion_mat[i, j]), va='center', ha='center', color='red', fontsize=10)\n",
    "    # 保存\n",
    "    plt.savefig(os.path.join(out_dir, \"Confusion_Matrix_{}.png\".format(set_name)))\n",
    "    plt.close()\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(cls_num):\n",
    "            print('class:{:<10}, total num:{:<6}, correct num:{:<5}  Recall: {:.2%} Precision: {:.2%}'.format(\n",
    "                classes[i], np.sum(confusion_mat[i, :]), confusion_mat[i, i],\n",
    "                confusion_mat[i, i] / (.1 + np.sum(confusion_mat[i, :])),\n",
    "                confusion_mat[i, i] / (.1 + np.sum(confusion_mat[:, i]))))\n",
    "\n",
    "\n",
    "def plot_line(train_x, train_y, valid_x, valid_y, mode, out_dir):\n",
    "    \"\"\"\n",
    "    绘制训练和验证集的loss曲线/acc曲线\n",
    "    :param train_x: epoch\n",
    "    :param train_y: 标量值\n",
    "    :param valid_x:\n",
    "    :param valid_y:\n",
    "    :param mode:  'loss' or 'acc'\n",
    "    :param out_dir:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    plt.plot(train_x, train_y, label='Train')\n",
    "    plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "    plt.ylabel(str(mode))\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    location = 'upper right' if mode == 'loss' else 'upper left'\n",
    "    plt.legend(loc=location)\n",
    "\n",
    "    plt.title('_'.join([mode]))\n",
    "    plt.savefig(os.path.join(out_dir, mode + '.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, path_log):\n",
    "        log_name = os.path.basename(path_log)\n",
    "        self.log_name = log_name if log_name else \"root\"\n",
    "        self.out_path = path_log\n",
    "\n",
    "        log_dir = os.path.dirname(self.out_path)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "    def init_logger(self):\n",
    "        logger = logging.getLogger(self.log_name)\n",
    "        logger.setLevel(level=logging.INFO)\n",
    "\n",
    "        # 配置文件Handler\n",
    "        file_handler = logging.FileHandler(self.out_path, 'w')\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        # 配置屏幕Handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        # console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "        # 添加handler\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "\n",
    "def make_logger(out_dir):\n",
    "    \"\"\"\n",
    "    在out_dir文件夹下以当前时间命名，创建日志文件夹，并创建logger用于记录信息\n",
    "    :param out_dir: str\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    now_time = datetime.now()\n",
    "    time_str = datetime.strftime(now_time, '%m-%d_%H-%M')\n",
    "    log_dir = os.path.join(out_dir, time_str)  # 根据config中的创建时间作为文件夹名\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    # 创建logger\n",
    "    path_log = os.path.join(log_dir, \"log.log\")\n",
    "    logger = Logger(path_log)\n",
    "    logger = logger.init_logger()\n",
    "    return logger, log_dir\n",
    "\n",
    "\n",
    "\n",
    "def mkdirs(paths):\n",
    "    if isinstance(paths, list) and not isinstance(paths, str):\n",
    "        for path in paths:\n",
    "            mkdir(path)\n",
    "    else:\n",
    "        mkdir(paths)\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def get_last_path(path, session):\n",
    "\tx = natsorted(glob(os.path.join(path,'*%s'%session)))[-1]\n",
    "\treturn x\n",
    "\n",
    "\n",
    "# def check_data_dir(path_tmp):\n",
    "#     assert os.path.exists(path_tmp), \\\n",
    "#         \"\\n\\n路径不存在，当前变量中指定的路径是：\\n{}\\n请检查相对路径的设置，或者文件是否存在\".format(os.path.abspath(path_tmp))\n",
    "def _upsample_like(src,tar):\n",
    "\n",
    "    # src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n",
    "    _,_,w,h=tar.size()\n",
    "    size=(w,h)\n",
    "    src=F.interpolate(src,size,mode='bilinear',align_corners=False)\n",
    "\n",
    "    return src\n",
    "\n",
    "def create_logger(BASE_DIR,name):\n",
    "    from datetime import datetime\n",
    "\n",
    "    now_time = datetime.now()\n",
    "    time_str = datetime.strftime(now_time, '%m-%d_%H-%M')\n",
    "    path_log = os.path.join(BASE_DIR, \"{}_{}log.log\".format(time_str,name))\n",
    "    logger = Logger(path_log)\n",
    "    logger = logger.init_logger()\n",
    "    return logger\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args, multiple):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 0.95 every 20 epochs\"\"\"\n",
    "    # lr = args.lr * (0.95 ** (epoch // 4))\n",
    "    lr = args.lr * (0.95 ** (epoch // 20))\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group['lr'] = lr * multiple[i]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    setup_seed(2)\n",
    "    print(np.random.randint(0, 10, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fee469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TransUnet_vit_seg_configs\n",
    "import ml_collections\n",
    "\n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "\n",
    "    config.classifier = 'seg'\n",
    "    config.representation_size = None\n",
    "    config.resnet_pretrained_path = None\n",
    "    config.pretrained_path = '../model/vit_checkpoint/imagenet21k/ViT-B_16.npz'\n",
    "    config.patch_size = 16\n",
    "\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.n_classes = 2\n",
    "    config.activation = 'softmax'\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_testing():\n",
    "    \"\"\"Returns a minimal configuration for testing.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 1\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 1\n",
    "    config.transformer.num_heads = 1\n",
    "    config.transformer.num_layers = 1\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    return config\n",
    "\n",
    "def get_r50_b16_config():\n",
    "    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n",
    "    config = get_b16_config()\n",
    "    config.patches.grid = (16, 16)\n",
    "    config.resnet = ml_collections.ConfigDict()\n",
    "    config.resnet.num_layers = (3, 4, 9)\n",
    "    config.resnet.width_factor = 1\n",
    "\n",
    "    config.classifier = 'seg'\n",
    "    # config.pretrained_path = '/home/wangjing/code/sesimic/swinunet/pretrained_ckpt/R50+ViT-B_16.npz'\n",
    "    config.pretrained_path = '/home/user/data/liuyue/active_learning_transformer/pretrainmodel/imagenet21k_R50+ViT-B_16.npz'\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.skip_channels = [512, 256, 64, 16]\n",
    "    config.n_classes = 1\n",
    "    config.n_skip = 3\n",
    "    config.activation = 'softmax'\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_b32_config():\n",
    "    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n",
    "    config = get_b16_config()\n",
    "    config.patches.size = (32, 32)\n",
    "    config.pretrained_path = '../model/vit_checkpoint/imagenet21k/ViT-B_32.npz'\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_l16_config():\n",
    "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 1024\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 4096\n",
    "    config.transformer.num_heads = 16\n",
    "    config.transformer.num_layers = 24\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.representation_size = None\n",
    "\n",
    "    # custom\n",
    "    config.classifier = 'seg'\n",
    "    config.resnet_pretrained_path = None\n",
    "    config.pretrained_path = '../model/vit_checkpoint/imagenet21k/ViT-L_16.npz'\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.n_classes = 2\n",
    "    config.activation = 'softmax'\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_r50_l16_config():\n",
    "    \"\"\"Returns the Resnet50 + ViT-L/16 configuration. customized \"\"\"\n",
    "    config = get_l16_config()\n",
    "    config.patches.grid = (16, 16)\n",
    "    config.resnet = ml_collections.ConfigDict()\n",
    "    config.resnet.num_layers = (3, 4, 9)\n",
    "    config.resnet.width_factor = 1\n",
    "\n",
    "    config.classifier = 'seg'\n",
    "    config.resnet_pretrained_path = '../model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz'\n",
    "    config.decoder_channels = (256, 128, 64, 16)\n",
    "    config.skip_channels = [512, 256, 64, 16]\n",
    "    config.n_classes = 2\n",
    "    config.activation = 'softmax'\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_l32_config():\n",
    "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
    "    config = get_l16_config()\n",
    "    config.patches.size = (32, 32)\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_h14_config():\n",
    "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n",
    "    config.hidden_size = 1280\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 5120\n",
    "    config.transformer.num_heads = 16\n",
    "    config.transformer.num_layers = 32\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b91fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TransUnet_vit_seg_modeling_resnet_skip\n",
    "import math\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "class StdConv2d(nn.Conv2d):\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
    "                        self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=bias, groups=groups)\n",
    "\n",
    "\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation (v2) bottleneck block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "        super().__init__()\n",
    "        cout = cout or cin\n",
    "        cmid = cmid or cout//4\n",
    "\n",
    "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n",
    "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
    "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if (stride != 1 or cin != cout):\n",
    "            # Projection also with pre-activation according to paper.\n",
    "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
    "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.gn_proj(residual)\n",
    "\n",
    "        # Unit's branch\n",
    "        y = self.relu(self.gn1(self.conv1(x)))\n",
    "        y = self.relu(self.gn2(self.conv2(y)))\n",
    "        y = self.gn3(self.conv3(y))\n",
    "\n",
    "        y = self.relu(residual + y)\n",
    "        return y\n",
    "\n",
    "    def load_from(self, weights, n_block, n_unit):\n",
    "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
    "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
    "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
    "\n",
    "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
    "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
    "\n",
    "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
    "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
    "\n",
    "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
    "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
    "\n",
    "        self.conv1.weight.copy_(conv1_weight)\n",
    "        self.conv2.weight.copy_(conv2_weight)\n",
    "        self.conv3.weight.copy_(conv3_weight)\n",
    "\n",
    "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
    "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
    "\n",
    "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
    "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
    "\n",
    "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
    "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
    "\n",
    "        if hasattr(self, 'downsample'):\n",
    "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
    "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
    "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
    "\n",
    "            self.downsample.weight.copy_(proj_conv_weight)\n",
    "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
    "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n",
    "\n",
    "class ResNetV2(nn.Module):\n",
    "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
    "\n",
    "    def __init__(self, block_units, width_factor):\n",
    "        super().__init__()\n",
    "        width = int(64 * width_factor)\n",
    "        self.width = width\n",
    "\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
    "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
    "                ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
    "                ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
    "                ))),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        b, c, in_size, _ = x.size()\n",
    "        x = self.root(x)\n",
    "        features.append(x)\n",
    "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)\n",
    "        for i in range(len(self.body)-1):\n",
    "            x = self.body[i](x)\n",
    "            right_size = int(in_size / 4 / (i+1))\n",
    "            if x.size()[2] != right_size:\n",
    "                pad = right_size - x.size()[2]\n",
    "                assert pad < 3 and pad > 0, \"x {} should {}\".format(x.size(), right_size)\n",
    "                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n",
    "                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n",
    "            else:\n",
    "                feat = x\n",
    "            features.append(feat)\n",
    "        x = self.body[-1](x)\n",
    "        return x, features[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7479a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0] set to: F:\\active learning\\URAL\n",
      "loaded: <module 'TransUnet_vit_seg_configs' from 'F:\\\\active learning\\\\URAL\\\\TransUnet_vit_seg_configs.py'> from F:\\active learning\\URAL\\TransUnet_vit_seg_configs.py\n"
     ]
    }
   ],
   "source": [
    "# 最直接、通常能解决问题的做法\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path.cwd().parent.resolve()\n",
    "sys.path.insert(0, str(p))\n",
    "print(\"sys.path[0] set to:\", sys.path[0])\n",
    "\n",
    "import TransUnet_vit_seg_configs as configs\n",
    "print(\"loaded:\", configs, \"from\", configs.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eb41fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TransUnet\n",
    "# coding=utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "import TransUnet_vit_seg_configs as configs\n",
    "# from TransUnet_vit_seg_modeling_resnet_skip import  ResNetV2\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\"\n",
    "\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        self.config = config\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:   # ResNet\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            patch_size_real = (patch_size[0] * 16, patch_size[1] * 16)\n",
    "            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])\n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.hidden_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hybrid:\n",
    "            x, features = self.hybrid_model(x)\n",
    "        else:\n",
    "            features = None\n",
    "        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings, features\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return encoded, attn_weights\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output, features = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n",
    "        return encoded, attn_weights, features\n",
    "\n",
    "\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            skip_channels=0,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.up(x)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        super().__init__(conv2d, upsampling)\n",
    "\n",
    "\n",
    "class DecoderCup(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        head_channels = 512\n",
    "        self.conv_more = Conv2dReLU(\n",
    "            config.hidden_size,\n",
    "            head_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=True,\n",
    "        )\n",
    "        decoder_channels = config.decoder_channels\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        if self.config.n_skip != 0:\n",
    "            skip_channels = self.config.skip_channels\n",
    "            for i in range(4-self.config.n_skip):  # re-select the skip channels according to n_skip\n",
    "                skip_channels[3-i]=0\n",
    "\n",
    "        else:\n",
    "            skip_channels=[0,0,0,0]\n",
    "\n",
    "        blocks = [\n",
    "            DecoderBlock(in_ch, out_ch, sk_ch) for in_ch, out_ch, sk_ch in zip(in_channels, out_channels, skip_channels)\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, hidden_states, features=None):\n",
    "        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, h, w)\n",
    "        x = self.conv_more(x)\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            if features is not None:\n",
    "                skip = features[i] if (i < self.config.n_skip) else None\n",
    "            else:\n",
    "                skip = None\n",
    "            x = decoder_block(x, skip=skip)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=1, zero_head=False, vis=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "        self.decoder = DecoderCup(config)\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=config['decoder_channels'][-1],\n",
    "            out_channels=config['n_classes'],\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "        x, attn_weights, features = self.transformer(x)  # (B, n_patch, hidden)\n",
    "        x = self.decoder(x, features)\n",
    "        logits = self.segmentation_head(x)\n",
    "        # logits=torch.sigmoid(logits)\n",
    "        return logits\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            res_weight = weights\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
    "\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            elif posemb.size()[1]-1 == posemb_new.size()[1]:\n",
    "                posemb = posemb[:, 1:]\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
    "                ntok_new = posemb_new.size(1)\n",
    "                if self.classifier == \"seg\":\n",
    "                    _, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)  # th2np\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = posemb_grid\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            # Encoder whole\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(res_weight[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(res_weight[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(res_weight, n_block=bname, n_unit=uname)\n",
    "\n",
    "CONFIGS = {\n",
    "    'ViT-B_16': configs.get_b16_config(),\n",
    "    'ViT-B_32': configs.get_b32_config(),\n",
    "    'ViT-L_16': configs.get_l16_config(),\n",
    "    'ViT-L_32': configs.get_l32_config(),\n",
    "    'ViT-H_14': configs.get_h14_config(),\n",
    "    'R50-ViT-B_16': configs.get_r50_b16_config(),\n",
    "    'R50-ViT-L_16': configs.get_r50_l16_config(),\n",
    "    'testing': configs.get_testing(),\n",
    "}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     vit_name=\"R50-ViT-B_16\"\n",
    "#     img_size=224\n",
    "#     vit_patches_size=16\n",
    "#     config_vit = CONFIGS[vit_name]\n",
    "#     if vit_name.find('R50') != -1:\n",
    "#         config_vit.patches.grid = (\n",
    "#         int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "#     model=VisionTransformer(config_vit)\n",
    "#     model.load_from(weights=np.load(config_vit.pretrained_path))\n",
    "#     x=torch.randn(3,1,224,224)\n",
    "#     y=model(x)\n",
    "#     print(y.shape)\n",
    "    # ops, params = get_model_complexity_info(model, (1, 224, 224), as_strings=True, print_per_layer_stat=True,\n",
    "    #                                         verbose=True)\n",
    "\n",
    "    # print(ops, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba8cf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictTimeSlice_transunet\n",
    "# from image_tools import *\n",
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "# from nets_copy import  THEBE_Net\n",
    "# from configs.config import get_config\n",
    "# from TransUnet import VisionTransformer\n",
    "import TransUnet_vit_seg_configs as configs\n",
    "\n",
    "CONFIGS = {\n",
    "    'ViT-B_16': configs.get_b16_config(),\n",
    "    'ViT-B_32': configs.get_b32_config(),\n",
    "    'ViT-L_16': configs.get_l16_config(),\n",
    "    'ViT-L_32': configs.get_l32_config(),\n",
    "    'ViT-H_14': configs.get_h14_config(),\n",
    "    'R50-ViT-B_16': configs.get_r50_b16_config(),\n",
    "    'R50-ViT-L_16': configs.get_r50_l16_config(),\n",
    "    'testing': configs.get_testing(),\n",
    "}\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "class faultsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,preprocessed_images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.images = preprocessed_images\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = TF.to_tensor(image)\n",
    "        image=norm(image)\n",
    "        image = TF.normalize(image, [4.0902375e-05, ], [0.0383472, ])\n",
    "        return image\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def predict_slice(model_name,seis,strategy_name,seed,otherchoice):\n",
    "    Z, XL = seis.shape\n",
    "    batch_size=8\n",
    "    im_height = Z\n",
    "    im_width = XL\n",
    "    splitsize = 224  # 96\n",
    "    stepsize = 112  # overlap half\n",
    "    overlapsize = splitsize - stepsize\n",
    "\n",
    "    horizontal_splits_number = int(np.ceil((im_width) / stepsize))\n",
    "    width_after_pad = stepsize * horizontal_splits_number + 2 * overlapsize\n",
    "    left_pad = int((width_after_pad - im_width) / 2)\n",
    "    right_pad = width_after_pad - im_width - left_pad\n",
    "\n",
    "    vertical_splits_number = int(np.ceil((im_height) / stepsize))\n",
    "    height_after_pad = stepsize * vertical_splits_number + 2 * overlapsize\n",
    "\n",
    "    top_pad = int((height_after_pad - im_height) / 2)\n",
    "    bottom_pad = height_after_pad - im_height - top_pad\n",
    "\n",
    "    horizontal_splits_number = horizontal_splits_number + 1\n",
    "    vertical_splits_number = vertical_splits_number + 1\n",
    "\n",
    "    X_list = []\n",
    "\n",
    "    X_list.extend(\n",
    "        split_Image(seis, True, top_pad, bottom_pad, left_pad, right_pad, splitsize, stepsize, vertical_splits_number,\n",
    "                    horizontal_splits_number))\n",
    "\n",
    "    X = np.asarray(X_list)\n",
    "\n",
    "    faults_dataset_test = faultsDataset(X)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=faults_dataset_test,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False)\n",
    "    # 加载模型\n",
    "    test_predictions = []\n",
    "    imageNo = -1\n",
    "    mergemethod = \"smooth\"\n",
    "    # model=create_model_thebe(model_name)\n",
    "    # cfg = get_config()\n",
    "    # imgsize = 224\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model=THEBE_Net(cfg,imgsize).to(device)\n",
    "\n",
    "    vit_name=\"R50-ViT-B_16\"\n",
    "    img_size=224\n",
    "    vit_patches_size=16\n",
    "    config_vit = CONFIGS[vit_name]\n",
    "    if vit_name.find('R50') != -1:\n",
    "            config_vit.patches.grid = (\n",
    "            int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "    model=VisionTransformer(config_vit).to(device)\n",
    "\n",
    "    model_nestunet_path = \"F:/active learning/active_learning_data/{}_{}/{}/SSL_checkpoint_best.pkl\".format(seed,otherchoice,strategy_name)\n",
    "    \n",
    "   \n",
    "    weights = torch.load(model_nestunet_path, map_location=\"cuda\")['model_state_dict']\n",
    "    weights_dict = {}\n",
    "    for k, v in weights.items():\n",
    "            new_k = k.replace('module.', '') if 'module' in k else k\n",
    "            weights_dict[new_k] = v\n",
    "    model.load_state_dict(weights_dict)\n",
    "\n",
    "    model.eval()\n",
    "    for images in test_loader:\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        y_preds=outputs.squeeze(1)\n",
    "        test_predictions.extend(y_preds.detach().cpu())\n",
    "        # print(y_preds.shape)\n",
    "        if len(test_predictions) >= vertical_splits_number * horizontal_splits_number:\n",
    "            imageNo = imageNo + 1\n",
    "            tosave = torch.stack(test_predictions).detach().cpu().numpy()[\n",
    "                     0:vertical_splits_number * horizontal_splits_number]\n",
    "            test_predictions = test_predictions[vertical_splits_number * horizontal_splits_number:]\n",
    "\n",
    "            if mergemethod == \"smooth\":\n",
    "                WINDOW_SPLINE_2D = window_2D(window_size=splitsize, power=2)\n",
    "                # add one dimension\n",
    "                tosave = np.expand_dims(tosave, -1)\n",
    "                tosave = np.array([patch * WINDOW_SPLINE_2D for patch in tosave])  # 224,224,450\n",
    "                tosave = tosave.reshape((vertical_splits_number, horizontal_splits_number, splitsize, splitsize, 1))\n",
    "                recover_Y_test_pred = recover_Image(tosave, (im_height, im_width, 1), left_pad, right_pad, top_pad,\n",
    "                                                    bottom_pad, overlapsize)\n",
    "\n",
    "    return recover_Y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "634bcae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_test_transunet\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "# from scheduler import GradualWarmupScheduler\n",
    "# from common_tools import create_logger \n",
    "# import losses\n",
    "import os\n",
    "import cv2\n",
    "import cmapy\n",
    "import matplotlib.pyplot as plt\n",
    "# from evalution_segmentaion import Evaluator\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# from image_tools import *\n",
    "# from predictTimeSlice import predict_slice\n",
    "\n",
    "import torch.utils.data\n",
    "import time\n",
    "\n",
    "# from evalution_segmentaion import Evaluator\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "# from configs.config import get_config\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import timm.models.vision_transformer\n",
    "# from predictTimeSlice import *\n",
    "# from predictTimeSlice_transunet import *\n",
    "\n",
    "# from TransUnet import VisionTransformer\n",
    "import TransUnet_vit_seg_configs as configs\n",
    "\n",
    "CONFIGS = {\n",
    "    'ViT-B_16': configs.get_b16_config(),\n",
    "    'ViT-B_32': configs.get_b32_config(),\n",
    "    'ViT-L_16': configs.get_l16_config(),\n",
    "    'ViT-L_32': configs.get_l32_config(),\n",
    "    'ViT-H_14': configs.get_h14_config(),\n",
    "    'R50-ViT-B_16': configs.get_r50_b16_config(),\n",
    "    'R50-ViT-L_16': configs.get_r50_l16_config(),\n",
    "    'testing': configs.get_testing(),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class Net:\n",
    "    def __init__(self, net, params, device):\n",
    "        self.net = net\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "\n",
    "    def train_before(self, train_data,val_data,n,strategy_name,seed,otherchoice):\n",
    "        logger = create_logger(\"./active_learning_data/{}_{}/{}/log\".format(seed,otherchoice,strategy_name),\"train_{}\".format(n))\n",
    "        vit_name=\"R50-ViT-B_16\"\n",
    "        img_size=224\n",
    "        vit_patches_size=16\n",
    "        config_vit = CONFIGS[vit_name]\n",
    "        if vit_name.find('R50') != -1:\n",
    "            config_vit.patches.grid = (\n",
    "            int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "\n",
    "        self.clf=VisionTransformer(config_vit).to(self.device)\n",
    "        # self.clf.load_from(weights=np.load(config_vit.pretrained_path))\n",
    "        \n",
    "\n",
    "        best_miou=0\n",
    "        \n",
    "        \n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        dice_loss = losses.DiceLoss(2)\n",
    "        mse_loss=nn.MSELoss()\n",
    "        \n",
    "         \n",
    "\n",
    "        n_epoch = self.params['n_epoch']\n",
    "        \n",
    "        \n",
    "        mean_train_losses = []\n",
    "        mean_val_losses = []\n",
    "        mean_train_accuracies = []\n",
    "        mean_val_accuracies = []\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "       \n",
    "        optimizer = optim.AdamW(self.clf.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-8, \n",
    "                                weight_decay=0.001)    #0.0004\n",
    "       \n",
    "        # 定义 Warmup 学习率策略\n",
    "        warmup_epochs = 10\n",
    "        warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lr_lambda=lambda epoch: (epoch + 1) / warmup_epochs if epoch < warmup_epochs else 1\n",
    "        )\n",
    "\n",
    "        # 定义余弦退火学习率策略\n",
    "        cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100 - warmup_epochs,eta_min=1e-6)\n",
    "        \n",
    "\n",
    "\n",
    "        train_loader = DataLoader(train_data, shuffle=True, **self.params['train_args'])\n",
    "        val_loader = DataLoader(val_data, shuffle=False, **self.params['val_args'])\n",
    "        \n",
    "        for epoch in tqdm(range(1, n_epoch+1), ncols=100):\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            train_accuracies = []\n",
    "            val_accuracies = []\n",
    "            self.clf.train()\n",
    "            for batch_idx, (x, y, idxs) in enumerate(train_loader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                out = self.clf(x)  #16,2,128,128\n",
    "                outputs=torch.zeros([out.size(0),2,224,224])\n",
    "                outputs[:,1,:,:]=out.squeeze(1)\n",
    "                outputs[:,0,:,:]=1-out.squeeze(1)\n",
    "                predicted_mask = out > 0.5\n",
    "               \n",
    "                tloss_ce = criterion(outputs.to(self.device),y.squeeze(1).long())\n",
    "                tloss_dice = dice_loss(outputs.to(self.device), y)\n",
    "                \n",
    "                tloss_mse=mse_loss(outputs[:,0,:,:].to(self.device),1-y)\n",
    "                \n",
    "                tloss=tloss_ce+tloss_dice+ tloss_mse\n",
    "                logger.info(\"Epoch {}: tloss_ce: {:.4f},tloss_mse:{:.4f},,tloss_dice: {:.4f}\".format(epoch, tloss_ce.item(),tloss_mse.item(),tloss_dice.item()))#\n",
    "                \n",
    "                \n",
    "               \n",
    "                tloss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_losses.append(tloss.data)\n",
    "                \n",
    "                train_acc = iou_pytorch(predicted_mask.squeeze(1).byte(), y.squeeze(1).byte(),1e-6)\n",
    "                train_accuracies.append(train_acc.mean())\n",
    "                \n",
    "                logger.info(\"Epoch {}: Acc: {:.2%},Loss: {:.4f}\".format(epoch, \n",
    "                                                                            train_acc.mean().item(),tloss.item()))\n",
    "            \n",
    "            if epoch < warmup_epochs:\n",
    "                    warmup_scheduler.step()\n",
    "            else:\n",
    "                    cosine_scheduler.step()\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            logger.info(f\"Epoch {epoch+1}, Learning Rate: {current_lr}\")\n",
    "\n",
    "          \n",
    "            self.clf.eval()\n",
    "            for x, y, idxs in val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out = self.clf(x)  #16,1,224,224\n",
    "                \n",
    "                outputs=torch.zeros([out.size(0),2,224,224])\n",
    "                outputs[:,1,:,:]=out.squeeze(1)\n",
    "                outputs[:,0,:,:]=1-out.squeeze(1)\n",
    "                \n",
    "                predicted_mask = out > 0.5\n",
    "                vloss_ce = criterion(outputs.to(self.device),y.squeeze(1).long())\n",
    "                vloss_dice = dice_loss(outputs.to(self.device), y)\n",
    "                \n",
    "                vloss_mse=mse_loss(outputs[:,0,:,:].to(self.device),1-y)\n",
    "                \n",
    "                vloss=vloss_ce+vloss_dice+vloss_mse\n",
    "                \n",
    "                logger.info(\"Epoch {}: vloss_ce: {:.4f},vloss_mse:{:.4f},vloss_dice: {:.6f}\".format(epoch, vloss_ce.item(),vloss_mse.item(),vloss_dice.item()))#\n",
    "                \n",
    "                val_losses.append(vloss.data)\n",
    "                val_acc = iou_pytorch(predicted_mask.squeeze(1).byte(), y.squeeze(1).byte(),1e-6)\n",
    "                logger.info(\"idx {}: Acc: {:.2%},loss:{}\".format(idxs, val_acc.mean().item(),vloss.mean()))\n",
    "                val_accuracies.append(val_acc.mean())\n",
    "            \n",
    "            mean_train_losses.append(torch.mean(torch.stack(train_losses)))\n",
    "            mean_val_losses.append(torch.mean(torch.stack(val_losses)))\n",
    "            mean_train_accuracies.append(torch.mean(torch.stack(train_accuracies)))\n",
    "            mean_val_accuracies.append(torch.mean(torch.stack(val_accuracies)))\n",
    "            val_iou = torch.mean(torch.stack(val_accuracies))    \n",
    "            logger.info('Epoch: {}. Train Loss: {:.4f}. Val Loss: {:.4f}. Train IoU: {:.4f}. Val IoU: {:.4f}. '\n",
    "                .format(epoch , torch.mean(torch.stack(train_losses)), torch.mean(torch.stack(val_losses)),\n",
    "                        torch.mean(torch.stack(train_accuracies)),val_iou))\n",
    "            \n",
    "            if best_miou < val_iou.item() :\n",
    "\n",
    "                best_miou = val_iou.item() \n",
    "                checkpoint = {\"model_state_dict\": self.clf.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"best_miou\": best_miou}\n",
    "                pkl_name = \"SSL_checkpoint_best.pkl\"\n",
    "\n",
    "                \n",
    "\n",
    "                path_checkpoint = os.path.join(\"./active_learning_data/{}_{}/{}\".format(seed,otherchoice,strategy_name), pkl_name)\n",
    "                torch.save(checkpoint, path_checkpoint)\n",
    "                logger.info(\"best_miou is :{}\".format(best_miou))\n",
    "\n",
    "                \n",
    "            # if epoch==100:\n",
    "                img=predicted_mask.squeeze(1)[0,:,:].cpu()\n",
    "                plt.imshow(img)\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/picture/val/{}_{}.png\".format(seed,otherchoice,strategy_name,n,int(idxs[0])))\n",
    "\n",
    "                mask=y.squeeze(1)[0,:,:].cpu()\n",
    "                plt.imshow(mask)\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/picture/val/{}_{}_mask.png\".format(seed,otherchoice,strategy_name,n,int(idxs[0])))\n",
    "        # print (best_miou)\n",
    "        return best_miou \n",
    "\n",
    "\n",
    "    def train(self, train_data,val_data,n,strategy_name,best_iou,seed,otherchoice):\n",
    "        logger = create_logger(\"./data/liuyue/active_learning_data/{}_{}/{}/log\".format(seed,otherchoice,strategy_name),\"train_{}\".format(n))\n",
    "        \n",
    "        vit_name=\"R50-ViT-B_16\"\n",
    "        img_size=224\n",
    "        vit_patches_size=16\n",
    "        config_vit = CONFIGS[vit_name]\n",
    "        if vit_name.find('R50') != -1:\n",
    "            config_vit.patches.grid = (\n",
    "            int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "        self.clf=VisionTransformer(config_vit).to(self.device)\n",
    "        model_nestunet_path = \"./active_learning_data/{}_{}/{}/SSL_checkpoint_best.pkl\".format(seed,otherchoice,strategy_name)\n",
    "        weights = torch.load(model_nestunet_path, map_location=\"cuda\")['model_state_dict']\n",
    "        weights_dict = {}\n",
    "        for k, v in weights.items():\n",
    "                new_k = k.replace('module.', '') if 'module' in k else k\n",
    "                weights_dict[new_k] = v\n",
    "        self.clf.load_state_dict(weights_dict)\n",
    "\n",
    "        best_miou=best_iou\n",
    "        print(best_iou)\n",
    "        print(best_miou)\n",
    "        \n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        dice_loss = losses.DiceLoss(2)\n",
    "        mse_loss=nn.MSELoss()\n",
    "        \n",
    "\n",
    "        n_epoch = self.params['n_epoch']\n",
    "        \n",
    "        \n",
    "        mean_train_losses = []\n",
    "        mean_val_losses = []\n",
    "        mean_train_accuracies = []\n",
    "        mean_val_accuracies = []\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # optimizer = optim.SGD(self.clf.parameters(), **self.params['optimizer_args'])\n",
    "        optimizer = optim.AdamW(self.clf.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8,\n",
    "                                weight_decay=0.001)\n",
    "        # optimizer = optim.Adam(self.clf.parameters(), lr=0.00001,eps=1e-4)\n",
    "        \n",
    "        # 定义 Warmup 学习率策略\n",
    "        warmup_epochs = 10\n",
    "        warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lr_lambda=lambda epoch: (epoch + 1) / warmup_epochs if epoch < warmup_epochs else 1\n",
    "        )\n",
    "\n",
    "        # 定义余弦退火学习率策略\n",
    "        cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100 - warmup_epochs,eta_min=1e-6)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        train_loader = DataLoader(train_data, shuffle=True, **self.params['train_args'])\n",
    "        val_loader = DataLoader(val_data, shuffle=False, **self.params['val_args'])\n",
    "        # trloss=[]\n",
    "        # # my_list = list(range(100))\n",
    "        # xlable=0\n",
    "        for epoch in tqdm(range(1, n_epoch+1), ncols=100):\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            train_accuracies = []\n",
    "            val_accuracies = []\n",
    "            self.clf.train()\n",
    "            for batch_idx, (x, y, idxs) in enumerate(train_loader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                out = self.clf(x)\n",
    "                # print(out.cpu().shape)\n",
    "                outputs=torch.zeros([out.size(0),2,224,224])\n",
    "                outputs[:,1,:,:]=out.squeeze(1)\n",
    "                outputs[:,0,:,:]=1-out.squeeze(1)\n",
    "                \n",
    "                predicted_mask = out > 0.5\n",
    "                tloss_ce = criterion(outputs.to(self.device),y.squeeze(1).long())\n",
    "                \n",
    "                tloss_dice =dice_loss(outputs.to(self.device), y)\n",
    "                \n",
    "                tloss_mse=mse_loss(outputs[:,0,:,:].to(self.device),1-y)\n",
    "               \n",
    "                tloss=tloss_ce+tloss_dice+ tloss_mse\n",
    "                logger.info(\"Epoch {}: tloss_ce: {:.4f},tloss_dice: {:.4f},tloss_mse:{:.4f}\".format(epoch, tloss_ce.item(),tloss_dice.item(),tloss_mse.item()))#\n",
    "                \n",
    "                tloss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_losses.append(tloss.data)\n",
    "                \n",
    "                train_acc = iou_pytorch(predicted_mask.squeeze(1).byte(), y.squeeze(1).byte(),1e-6)\n",
    "                train_accuracies.append(train_acc.mean())\n",
    "                \n",
    "                logger.info(\"Epoch {}: Acc: {:.2%},Loss: {:.4f}\".format(epoch, \n",
    "                                                                            train_acc.mean().item(), tloss.item()))\n",
    "\n",
    "            if epoch < warmup_epochs:\n",
    "                    warmup_scheduler.step()\n",
    "            else:\n",
    "                    cosine_scheduler.step()\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            logger.info(f\"Epoch {epoch+1}, Learning Rate: {current_lr}\")\n",
    "\n",
    "            # if epoch%10==0:\n",
    "            self.clf.eval()\n",
    "            for x, y, idxs in val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "             \n",
    "                out = self.clf(x)  #16,1,224,224\n",
    "                \n",
    "                outputs=torch.zeros([out.size(0),2,224,224])\n",
    "                outputs[:,1,:,:]=out.squeeze(1)\n",
    "                outputs[:,0,:,:]=1-out.squeeze(1)\n",
    "                \n",
    "                predicted_mask = out > 0.5\n",
    "                vloss_ce = criterion(outputs.to(self.device),y.squeeze(1).long())\n",
    "                \n",
    "                \n",
    "                vloss_mse=mse_loss(outputs[:,0,:,:].to(self.device),1-y)\n",
    "                vloss_dice = dice_loss(outputs.to(self.device),y)\n",
    "                \n",
    "                vloss=vloss_ce+vloss_dice+vloss_mse\n",
    "                logger.info(\"Epoch {}: vloss_ce: {:.4f},vloss_dice: {:.4f},vloss_mse:{:.4f}\".format(epoch, vloss_ce.item(),vloss_dice.item(),vloss_mse.item()))#\n",
    "                \n",
    "                val_losses.append(vloss.data)\n",
    "                val_acc = iou_pytorch(predicted_mask.squeeze(1).byte(), y.squeeze(1).byte(),1e-6)\n",
    "                logger.info(\"idx {}: Acc: {:.2%},loss:{}\".format(idxs, val_acc.mean().item(),vloss.mean()))\n",
    "                val_accuracies.append(val_acc.mean())\n",
    "            \n",
    "            mean_train_losses.append(torch.mean(torch.stack(train_losses)))\n",
    "            mean_val_losses.append(torch.mean(torch.stack(val_losses)))\n",
    "            mean_train_accuracies.append(torch.mean(torch.stack(train_accuracies)))\n",
    "            mean_val_accuracies.append(torch.mean(torch.stack(val_accuracies)))\n",
    "            val_iou = torch.mean(torch.stack(val_accuracies))    \n",
    "            logger.info('Epoch: {}. Train Loss: {:.4f}. Val Loss: {:.4f}. Train IoU: {:.4f}. Val IoU: {:.4f}. '\n",
    "                .format(epoch , torch.mean(torch.stack(train_losses)), torch.mean(torch.stack(val_losses)),\n",
    "                        torch.mean(torch.stack(train_accuracies)),val_iou))\n",
    "            \n",
    "            if best_miou < val_iou.item() :\n",
    "\n",
    "                best_miou = val_iou.item() \n",
    "                checkpoint = {\"model_state_dict\": self.clf.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"best_miou\": best_miou}\n",
    "                pkl_name = \"SSL_checkpoint_best.pkl\"\n",
    "\n",
    "                path_checkpoint = os.path.join(\"./active_learning_data/{}_{}/{}\".format(seed,otherchoice,strategy_name), pkl_name)\n",
    "                torch.save(checkpoint, path_checkpoint)\n",
    "                logger.info(\"best_miou is :{}\".format(best_miou))\n",
    "\n",
    "                \n",
    "            # if epoch==100:\n",
    "                img=predicted_mask.squeeze(1)[0,:,:].cpu()\n",
    "                plt.imshow(img)\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/picture/val/{}_{}.png\".format(seed,otherchoice,strategy_name,n,int(idxs[0])))\n",
    "\n",
    "                mask=y.squeeze(1)[0,:,:].cpu()\n",
    "                plt.imshow(mask)\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/picture/val/{}_{}_mask.png\".format(seed,otherchoice,strategy_name,n,int(idxs[0])))\n",
    "        # print (best_miou)\n",
    "        return best_miou\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def predict_prob_RandomSampling(self, data,n,seed,otherchoice,picknum,picknum_no,flag):\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['trainsmall_args'])\n",
    "        for x, y, idxs in loader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            new_train_imgs_small=np.zeros([picknum,224,224])\n",
    "            new_train_masks_small=np.zeros([picknum,224,224])\n",
    "            # fid=np.random.randint(64, 936, size=(50))\n",
    "            # sid = np.random.randint(64, 1936, size=(50))\n",
    "            fid=np.random.randint(112,400, size=(picknum))\n",
    "            sid = np.random.randint(112, 1936, size=(picknum))\n",
    "\n",
    "        # 找到所有值为 1 的索引\n",
    "            indices = np.where(flag == 1)[0]  # np.where 返回的是元组，选择第一个元素\n",
    "\n",
    "            # 从这些索引中随机选择一个索引\n",
    "            random_index = np.random.choice(indices)\n",
    "            flag[random_index]=0\n",
    "            image=x.squeeze(1)[random_index].cpu()\n",
    "            masks=y.squeeze(1)[random_index].cpu()\n",
    "            maskContour=[]\n",
    "            \n",
    "            for i in range(picknum):\n",
    "                firstid=fid[i]\n",
    "                secondid=sid[i]\n",
    "                maskContour.append((secondid,firstid))\n",
    "                for j in range(224):\n",
    "                    for z in range(224):\n",
    "                        new_train_imgs_small[i][j][z]=image[(firstid-112+j)][(secondid-112+z)]\n",
    "                        new_train_masks_small[i][j][z]=masks[(firstid-112+j)][(secondid-112+z)]\n",
    "            \n",
    "\n",
    "         # 克隆图像\n",
    "        resultImg = masks.numpy().copy()*255\n",
    "        resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "        # 创建一个彩色图像\n",
    "        m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # 遍历每个连通域点并绘制红色圆圈\n",
    "        for point in maskContour:\n",
    "            cv2.circle(m_resultImg, point, 1, (0, 0, 255), 10)  # 红色圆圈\n",
    "\n",
    "    \n",
    "\n",
    "        # 使用matplotlib显示图像\n",
    "        # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "        m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 显示图像\n",
    "        plt.imshow(m_resultImg_rgb)\n",
    "        # plt.axis('off')  # 不显示坐标轴\n",
    "        # plt.show()\n",
    "        plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/_mask_points.png\".format(seed,otherchoice,\"RandomSampling\",n))\n",
    "\n",
    "\n",
    "        if n==1:\n",
    "                new_train_imgs= new_train_imgs_small\n",
    "                new_train_masks=new_train_masks_small\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "        else:\n",
    "                imgbefor=np.load(\"./active_learning/data/THEBE_224/train_img_small.npy\")\n",
    "                maskbefor=np.load(\"./active_learning/data/THEBE_224/train_mask_small.npy\")\n",
    "                print(\"imgbefor:{}\".format(imgbefor.shape))\n",
    "                print(\"maskbefor:{}\".format(maskbefor.shape))\n",
    "                imgbefor=torch.tensor(imgbefor)\n",
    "                maskbefor=torch.tensor(maskbefor)\n",
    "                new_train_imgs_small=torch.tensor(new_train_imgs_small)\n",
    "                new_train_masks_small=torch.tensor(new_train_masks_small)\n",
    "                new_train_imgs= torch.cat((new_train_imgs_small, imgbefor), dim=0)\n",
    "                new_train_masks=torch.cat((new_train_masks_small, maskbefor), dim=0)\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))    \n",
    "        \n",
    "        np.save(\"./data/THEBE_224/train_img_small.npy\",new_train_imgs)\n",
    "        np.save(\"./data/THEBE_224/train_mask_small.npy\",new_train_masks)\n",
    "        \n",
    "        \n",
    "           \n",
    "        return random_index, flag \n",
    "      \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def predict_prob_MarginSampling(self, data,n,seed,otherchoice,picknum,picknum_no,flag):#最小   正常版本，，不变化\n",
    "        vit_name=\"R50-ViT-B_16\"\n",
    "        img_size=224\n",
    "        vit_patches_size=16\n",
    "        config_vit = CONFIGS[vit_name]\n",
    "        if vit_name.find('R50') != -1:\n",
    "            config_vit.patches.grid = (\n",
    "            int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "        self.clf=VisionTransformer(config_vit).to(self.device)\n",
    "\n",
    "        \n",
    "        # self.clf = self.net().to(self.device)\n",
    "        model_nestunet_path =  \"./active_learning_data/{}_{}/{}/SSL_checkpoint_best.pkl\".format(seed,otherchoice,\"MarginSampling\")\n",
    "        weights = torch.load(model_nestunet_path, map_location=\"cuda\")['model_state_dict']\n",
    "        weights_dict = {}\n",
    "        for k, v in weights.items():\n",
    "                new_k = k.replace('module.', '') if 'module' in k else k\n",
    "                weights_dict[new_k] = v\n",
    "        self.clf.load_state_dict(weights_dict)\n",
    "\n",
    "        self.clf.eval()\n",
    "                \n",
    "\n",
    "        loader = DataLoader(data, shuffle=False, **self.params['trainsmall_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                outputs=np.zeros([len(idxs),2,512,2048])\n",
    "        for idx in idxs:\n",
    "            recover_Y_test_pred=predict_slice(THEBE_Net, x[idx].squeeze().cpu(),\"MarginSampling\",seed,otherchoice)#512,2048,1\n",
    "            outputs[idx,1,:,:]=np.squeeze(recover_Y_test_pred)\n",
    "        outputs[:,0,:,:]=1-outputs[:,1,:,:]\n",
    "        outputs=torch.tensor(outputs)\n",
    "        predict= torch.argmax(outputs,dim=1) \n",
    "        num=abs(outputs[:,1,:,:]-outputs[:,0,:,:])\n",
    "        num=num.cpu()\n",
    "\n",
    "        data={}\n",
    "        for idx in idxs:\n",
    "            if flag[idx]==1:\n",
    "                points=min_50(num[idx])  #50个坐标\n",
    "                # print(points)\n",
    "                labels, centroids=kmeans(points)   #labels=0,1,2\n",
    "                ####################################可视化 聚类的点\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.scatter(points[:, 1], points[:, 0],s=10, c=labels, cmap='viridis')\n",
    "                plt.scatter(centroids[:, 1], centroids[:, 0], s=20, c='red', marker='X')  # 绘制簇中心\n",
    "                plt.title(\"K-means Clustering (K=3)\")\n",
    "                plt.xlabel(\"X\")\n",
    "                plt.ylabel(\"Y\")\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/picture{}_kmeans.png\".format(seed,otherchoice,\"MarginSampling\",n,idx))\n",
    "                plt.close()\n",
    "\n",
    "               \n",
    "                #############################创建字典\n",
    "                labels0=np.where(labels==0)   #索引值\n",
    "                labels1=np.where(labels==1)\n",
    "                labels2=np.where(labels==2)\n",
    "                point0=points[labels0]  #对应的区域点\n",
    "                point1=points[labels1]\n",
    "                point2=points[labels2]\n",
    "                area0=(point0[:,1].min(),point0[:,1].max())\n",
    "                area1=(point1[:,1].min(),point1[:,1].max())\n",
    "                area2=(point2[:,1].min(),point2[:,1].max())\n",
    "                # count0=len(labels0[0])\n",
    "                # count1=len(labels1[0])\n",
    "                # count2=len(labels2[0])\n",
    "                data[\"image_{}\".format(idx)]={\"area0\": area0 ,\"point0\": point0 ,\"count0\":0,\"area1\":  area1,\"point1\":  point1 ,\"count1\":0,\"area2\": area2  ,\"point2\": point2,\"count2\":0}                        \n",
    "               \n",
    "                # data[\"image_{}\".format(idx)]={\"area0\": area0 ,\"point0\": point0 ,\"count0\":count0,\"area1\":  area1,\"point1\":  point1 ,\"count1\":count1,\"area2\": area2  ,\"point2\": point2,\"count2\":count2}                        \n",
    "                # print(data) \n",
    "\n",
    "                ####################################可视化 pridect\n",
    "                # # 克隆图像\n",
    "                resultImg =predict[idx,:,:].cpu().numpy().copy()*255\n",
    "                resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "                # 创建一个彩色图像\n",
    "                m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                # 遍历每个连通域点并绘制红色圆圈\n",
    "                for point in point0:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (0, 0, 255), 15)  # 红色圆圈\n",
    "                for point in point1:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (0, 255, 0), 15)  # 红色圆圈\n",
    "                for point in point2:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (255, 255, 0), 15)  # 红色圆圈\n",
    "\n",
    "                \n",
    "                # 使用matplotlib显示图像\n",
    "                # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "                m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # 显示图像\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(m_resultImg_rgb)\n",
    "                # plt.axis('off')  # 不显示坐标轴\n",
    "                # plt.show()\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/picture{}_pridect_points.png\".format(seed,otherchoice,\"MarginSampling\",n,idx))\n",
    "                plt.close()\n",
    "                \n",
    "            else:\n",
    "                    data[\"image_{}\".format(idx)]={\"area0\":[] ,\"point0\": [] ,\"count0\":0,\"area1\":  [],\"point1\":  [] ,\"count1\":0,\"area2\": []  ,\"point2\": [],\"count2\":0}                        \n",
    "               \n",
    "                    continue\n",
    "        # print(data)     \n",
    "        # with open('/home/user/data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/data.txt'.format(seed,otherchoice,\"MarginSampling\",n), 'w') as f:\n",
    "        #     json.dump(data, f)\n",
    "        # ########################计算区域内标注的和\n",
    "        area_sum=torch.zeros([len(idxs),3])\n",
    "        for i in range(len(idxs)):\n",
    "            for j in range(3):\n",
    "                if flag[i]==1:\n",
    "                    left=data[\"image_{}\".format(i)][\"area{}\".format(j)][0]\n",
    "                    right=data[\"image_{}\".format(i)][\"area{}\".format(j)][1]\n",
    "                    sum=torch.sum(predict[i,:,left:right])\n",
    "                    area_sum[i,j]=sum\n",
    "                    data[\"image_{}\".format(i)][\"count{}\".format(j)]=sum\n",
    "\n",
    "        #############################找到和最大的1个区域\n",
    "        flattened_tensor = area_sum.flatten()\n",
    "        # 2. 获取最大的 1 个元素的索引\n",
    "        values, indices = torch.topk(flattened_tensor, 1, largest=True)\n",
    "        # 3. 将一维索引转换为二维坐标\n",
    "        # 使用 divmod 来获取行和列\n",
    "        rows, cols = np.divmod(indices, area_sum.size(1))  # tensor.size(1) 是列数\n",
    "        # 输出最大的 1个元素的坐标\n",
    "        coordinates = torch.stack((rows, cols), dim=1)  #each_count中的位置坐标\n",
    "        print(coordinates )     #a=tensor([[0, 0]])\n",
    "        # int(a[0][1]) =0\n",
    "        #############################找到点数最多的1个区域\n",
    "        # each_count=torch.zeros([len(idxs),3])\n",
    "        # for i in range(len(idxs)):\n",
    "        #     for j in range(3):\n",
    "        #             each_count[i,j]=data[\"image_{}\".format(i)][\"count{}\".format(j)]\n",
    "        # flattened_tensor = each_count.flatten()\n",
    "        # # 2. 获取最大的 1 个元素的索引\n",
    "        # values, indices = torch.topk(flattened_tensor, 1, largest=True)\n",
    "        # # 3. 将一维索引转换为二维坐标\n",
    "        # # 使用 divmod 来获取行和列\n",
    "        # rows, cols = np.divmod(indices, each_count.size(1))  # tensor.size(1) 是列数\n",
    "        # # 输出最大的 3个元素的坐标\n",
    "        # coordinates = torch.stack((rows, cols), dim=1)  #each_count中的位置坐标\n",
    "        # print(coordinates )     #a=tensor([[0, 0]])\n",
    "        # # int(a[0][1]) =0\n",
    "        ###############################找到点数最多的区域  -》标注\n",
    "        num_image_pick=int(coordinates[0][0])\n",
    "        num_area_pick=int(coordinates[0][1])\n",
    "        flag[ num_image_pick]=0\n",
    "        left=data[\"image_{}\".format( num_image_pick)][\"area{}\".format(num_area_pick)][0]   \n",
    "        right=data[\"image_{}\".format( num_image_pick)][\"area{}\".format(num_area_pick)][1]   \n",
    "        print(left,right)  #397,514\n",
    "        img_area=x.squeeze(1)[num_image_pick][:,left:right]\n",
    "        mask_area=y.squeeze(1)[num_image_pick][:,left:right]\n",
    "    ###################################对选定的区域进行可视化\n",
    "    # # 克隆图像\n",
    "        resultImg =predict[num_image_pick,:,:].cpu().numpy().copy()*255\n",
    "        resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "        # 创建一个彩色图像\n",
    "        m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        #绘制所选中区域\n",
    "        a=np.where( predict[num_image_pick,:,left:right]!=1)\n",
    "        b=(a[1]+int(left),a[0])\n",
    "        for i in range(b[0].size):\n",
    "            d=(b[0][i],b[1][i])\n",
    "            cv2.circle(m_resultImg, d, 1, (160,160,160), 1)\n",
    "\n",
    "        # 绘制三个区域点\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point0\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (0, 0, 255), 15)  # 红色圆圈\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point1\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (0, 255, 0), 15)  # 绿色圆圈\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point2\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (255, 255, 0), 15)  # 黄色圆圈\n",
    "\n",
    "        \n",
    "        # 使用matplotlib显示图像\n",
    "        # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "        m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 显示图像\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(m_resultImg_rgb)\n",
    "        # plt.axis('off')  # 不显示坐标轴\n",
    "        # plt.show()\n",
    "        plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/picture{}_pickarea_pridect_points.png\".format(seed,otherchoice,\"MarginSampling\",n,num_image_pick))\n",
    "        plt.close()\n",
    "\n",
    "        ###############################求这个区域的连通性\n",
    "        con_nums=[]\n",
    "        mask_area=mask_area.cpu().numpy().astype(np.uint8)\n",
    "        # 连通性分析\n",
    "        num_labels, labels = cv2.connectedComponents(mask_area, connectivity=8)\n",
    "        output_image = np.zeros((labels.shape[0], labels.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # 为每个连通组件指定不同的颜色\n",
    "        for label in range(1, num_labels):  # 0 是背景，跳过\n",
    "            con_nums.append(label)\n",
    "            output_image[labels == label] = np.random.randint(0, 255, 3)\n",
    "\n",
    "        # 显示图像\n",
    "        plt.imshow(output_image)\n",
    "        plt.axis('off')  # 不显示坐标轴\n",
    "        plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/area{}_connect.png\".format(seed,otherchoice,\"MarginSampling\",n,num_area_pick))\n",
    "        \n",
    "        np.savetxt('./active_learning_data/{}_{}/{}/pick/split_{}/area{}_labels.txt'.format(seed,otherchoice,\"MarginSampling\",n,num_area_pick), labels, fmt='%d', delimiter=',')\n",
    "        ######labels是一个矩阵，由0，1，2，3，，，，，类\n",
    "        #########################根据连通性切割小图，\n",
    "        new_train_imgs_small=torch.zeros([100,224,224])\n",
    "        new_train_masks_small=torch.zeros([100,224,224])\n",
    "        id=0\n",
    "        #################按照断层连通性\n",
    "        for label in range(1, num_labels):\n",
    "            fids,sids=np.where(labels==label)\n",
    "            # for ids in range(len(fids)):\n",
    "            #     maskContour.append((sids[ids],fids[ids]))\n",
    "            l=abs(fids[-1]-fids[0])\n",
    "            w=abs(sids[-1]-sids[0])\n",
    "            pickimg=np.zeros([l,w])\n",
    "            pickmask=np.zeros([l,w])\n",
    "            pickimg=img_area[min(fids[0],fids[-1]):max(fids[0],fids[-1]),min(sids[0],sids[-1]):max(sids[0],sids[-1])]\n",
    "            pickmask=mask_area[min(fids[0],fids[-1]):max(fids[0],fids[-1]),min(sids[0],sids[-1]):max(sids[0],sids[-1])]\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(pickimg.cpu())\n",
    "            \n",
    "            plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/{}_img.png\".format(seed,otherchoice,\"MarginSampling\",n,label))\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(pickmask)\n",
    "        \n",
    "            plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/{}_mask.png\".format(seed,otherchoice,\"MarginSampling\",n,label))\n",
    "        \n",
    "            \n",
    "            a=fids[0]\n",
    "            b=sids[0]\n",
    "            \n",
    "            if (sids[-1]-sids[0] )>0:\n",
    "                while(1):\n",
    "                    # print(a,b)\n",
    "                    if a>288 or b>(right-left-224):\n",
    "                        break\n",
    "                    new_train_imgs_small[id]=torch.tensor(img_area[a:a+224,b:b+224])\n",
    "                    new_train_masks_small[id]=torch.tensor(mask_area[a:a+224,b:b+224])\n",
    "                \n",
    "                    c=np.where(fids==a+56)\n",
    "                    if c[0].size==0:\n",
    "                        break\n",
    "                    else:\n",
    "                        a=fids[c[0][0]]\n",
    "                        b=sids[c[0][0]]\n",
    "                        id+=1\n",
    "                        # print(index)\n",
    "                   \n",
    "\n",
    "            else:\n",
    "                while(1):\n",
    "                    print(a,b)\n",
    "                    if a>288 or b<224:\n",
    "                        break\n",
    "                    new_train_imgs_small[id]=torch.tensor(img_area[a:a+224,b-224:b])\n",
    "                    new_train_masks_small[id]=torch.tensor(mask_area[a:a+224,b-224:b])\n",
    "                    \n",
    "                    c=np.where(fids==a+56)\n",
    "                    if c[0].size==0:\n",
    "                        break\n",
    "                    else:\n",
    "                        a=fids[c[0][0]]\n",
    "                        b=sids[c[0][0]]\n",
    "                        id+=1\n",
    "        print(id)\n",
    "        \n",
    "        #################按照patch\n",
    "        # for j in range(4):\n",
    "            # for k in range(int(right-left)//128):\n",
    "            #     img1=img_area[j*128:(j+1)*128,k*128:(k+1)*128]\n",
    "            #     mask1=mask_area[j*128:(j+1)*128,k*128:(k+1)*128]\n",
    "            #     if mask1.sum()!=0:\n",
    "            #         new_train_imgs_small[id]=torch.tensor(img1)\n",
    "            #         new_train_masks_small[id]=torch.tensor(mask1)\n",
    "            #         id+=1\n",
    "        # print(id)\n",
    "        \n",
    "\n",
    "        new_train_imgs_small=new_train_imgs_small[:id]\n",
    "        new_train_masks_small=new_train_masks_small[:id]\n",
    "        if n==1:\n",
    "                new_train_imgs= new_train_imgs_small\n",
    "                new_train_masks=new_train_masks_small\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "        else:\n",
    "                imgbefor=np.load(\"./data/THEBE_224/train_img_small.npy\")\n",
    "                maskbefor=np.load(\"./data/THEBE_224/train_mask_small.npy\")\n",
    "                print(\"imgbefor:{}\".format(imgbefor.shape))\n",
    "                print(\"maskbefor:{}\".format(maskbefor.shape))\n",
    "                imgbefor=torch.tensor(imgbefor)\n",
    "                maskbefor=torch.tensor(maskbefor)\n",
    "                new_train_imgs_small=torch.tensor(new_train_imgs_small)\n",
    "                new_train_masks_small=torch.tensor(new_train_masks_small)\n",
    "                new_train_imgs= torch.cat((new_train_imgs_small, imgbefor), dim=0)\n",
    "                new_train_masks=torch.cat((new_train_masks_small, maskbefor), dim=0)\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "        \n",
    "        \n",
    "    \n",
    "        np.save(\"./data/THEBE_224/train_img_small.npy\",new_train_imgs)\n",
    "        np.save(\"./data/THEBE_224/train_mask_small.npy\",new_train_masks)              \n",
    "        # print(flag)\n",
    "        return data,flag\n",
    "\n",
    "\n",
    "    def predict_prob_EntropySampling(self, data,n,seed,otherchoice,picknum,picknum_no,flag):\n",
    "        # self.clf = self.net().to(self.device)\n",
    "        vit_name=\"R50-ViT-B_16\"\n",
    "        img_size=224\n",
    "        vit_patches_size=16\n",
    "        config_vit = CONFIGS[vit_name]\n",
    "        if vit_name.find('R50') != -1:\n",
    "            config_vit.patches.grid = (\n",
    "            int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "        self.clf=VisionTransformer(config_vit).to(self.device)\n",
    "        model_nestunet_path = \"./active_learning_data/{}_{}/{}/SSL_checkpoint_best.pkl\".format(seed,otherchoice,\"EntropySampling\")\n",
    "        weights = torch.load(model_nestunet_path, map_location=\"cuda\")['model_state_dict']\n",
    "        weights_dict = {}\n",
    "        for k, v in weights.items():\n",
    "                new_k = k.replace('module.', '') if 'module' in k else k\n",
    "                weights_dict[new_k] = v\n",
    "        self.clf.load_state_dict(weights_dict)\n",
    "\n",
    "        self.clf.eval()\n",
    "        \n",
    "       \n",
    "        \n",
    "        loader = DataLoader(data, shuffle=False, **self.params['trainsmall_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "               \n",
    "                outputs=np.zeros([len(idxs),2,512,2048])\n",
    "        for idx in idxs:\n",
    "            recover_Y_test_pred=predict_slice(THEBE_Net, x[idx].squeeze().cpu(),\"EntropySampling\",seed,otherchoice)#512,2048,1\n",
    "            outputs[idx,1,:,:]=np.squeeze(recover_Y_test_pred)\n",
    "\n",
    "        outputs[:,0,:,:]=1-outputs[:,1,:,:]\n",
    "        outputs=torch.tensor(outputs)\n",
    "        \n",
    "        predict= torch.argmax(outputs,dim=1)             \n",
    "        # print(predict.shape)\n",
    "        num_0=outputs[:,0,:,:]\n",
    "        num_1=outputs[:,1,:,:]\n",
    "        num0_log=torch.log(num_0)\n",
    "        num1_log=torch.log(num_1)\n",
    "        num0_log = torch.nan_to_num(num0_log, nan=0.0)\n",
    "        num1_log = torch.nan_to_num(num1_log, nan=0.0)\n",
    "        entr0_log=num_0*num0_log\n",
    "        entr1_log=num_1*num1_log\n",
    "        \n",
    "        entr_sum=-entr0_log-entr1_log\n",
    "        entr_sum=entr_sum.cpu()\n",
    "\n",
    "        # print(entr_sum.max())\n",
    "        # flag=np.ones([len(idxs),512,2048],type=\"bool\")\n",
    "        data={}\n",
    "        for idx in idxs:\n",
    "            if flag[idx]==1:\n",
    "                points=max_50(entr_sum[idx])  #50个坐标\n",
    "                # print(points)\n",
    "                labels, centroids=kmeans(points)   #labels=0,1,2\n",
    "                ####################################可视化 聚类的点\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.scatter(points[:, 1], points[:, 0],s=10, c=labels, cmap='viridis')\n",
    "                plt.scatter(centroids[:, 1], centroids[:, 0], s=20, c='red', marker='X')  # 绘制簇中心\n",
    "                plt.title(\"K-means Clustering (K=3)\")\n",
    "                plt.xlabel(\"X\")\n",
    "                plt.ylabel(\"Y\")\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/picture{}_kmeans.png\".format(seed,otherchoice,\"EntropySampling\",n,idx))\n",
    "                plt.close()\n",
    "\n",
    "               \n",
    "                #############################创建字典\n",
    "                labels0=np.where(labels==0)   #索引值\n",
    "                labels1=np.where(labels==1)\n",
    "                labels2=np.where(labels==2)\n",
    "                point0=points[labels0]  #对应的区域点\n",
    "                point1=points[labels1]\n",
    "                point2=points[labels2]\n",
    "                area0=(point0[:,1].min(),point0[:,1].max())\n",
    "                area1=(point1[:,1].min(),point1[:,1].max())\n",
    "                area2=(point2[:,1].min(),point2[:,1].max())\n",
    "                \n",
    "\n",
    "                data[\"image_{}\".format(idx)]={\"area0\": area0 ,\"point0\": point0 ,\"count0\":0,\"area1\":  area1,\"point1\":  point1 ,\"count1\":0,\"area2\": area2  ,\"point2\": point2,\"count2\":0}                        \n",
    "                # print(data) \n",
    "\n",
    "                ####################################可视化 pridect\n",
    "                # # 克隆图像\n",
    "                resultImg =predict[idx,:,:].cpu().numpy().copy()*255\n",
    "                resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "                # 创建一个彩色图像\n",
    "                m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                # 遍历每个连通域点并绘制红色圆圈\n",
    "                for point in point0:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (0, 0, 255), 15)  # 红色圆圈\n",
    "                for point in point1:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (0, 255, 0), 15)  # 红色圆圈\n",
    "                for point in point2:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (255, 255, 0), 15)  # 红色圆圈\n",
    "\n",
    "                \n",
    "                # 使用matplotlib显示图像\n",
    "                # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "                m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # 显示图像\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(m_resultImg_rgb)\n",
    "                # plt.axis('off')  # 不显示坐标轴\n",
    "                # plt.show()\n",
    "                plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/picture{}_pridect_points.png\".format(seed,otherchoice,\"EntropySampling\",n,idx))\n",
    "                plt.close()\n",
    "                \n",
    "            else:\n",
    "                    data[\"image_{}\".format(idx)]={\"area0\":[] ,\"point0\": [] ,\"count0\":0,\"area1\":  [],\"point1\":  [] ,\"count1\":0,\"area2\": []  ,\"point2\": [],\"count2\":0}                        \n",
    "               \n",
    "                    continue\n",
    "        print(data)     \n",
    "        # with open('/home/user/data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/data.txt'.format(seed,otherchoice,\"EntropySampling\",n), 'w') as f:\n",
    "        #     json.dump(data, f)\n",
    "        # ########################计算区域内标注的和\n",
    "        area_sum=torch.zeros([len(idxs),3])\n",
    "        for i in range(len(idxs)):\n",
    "            for j in range(3):\n",
    "                if flag[i]==1:\n",
    "                    left=data[\"image_{}\".format(i)][\"area{}\".format(j)][0]\n",
    "                    right=data[\"image_{}\".format(i)][\"area{}\".format(j)][1]\n",
    "                    sum=torch.sum(predict[i,:,left:right])\n",
    "                    area_sum[i,j]=sum\n",
    "                    data[\"image_{}\".format(i)][\"count{}\".format(j)]=sum\n",
    "        #############################找到和最大的1个区域\n",
    "        flattened_tensor = area_sum.flatten()\n",
    "        # 2. 获取最大的 1 个元素的索引\n",
    "        values, indices = torch.topk(flattened_tensor, 1, largest=True)\n",
    "        # 3. 将一维索引转换为二维坐标\n",
    "        # 使用 divmod 来获取行和列\n",
    "        rows, cols = np.divmod(indices, area_sum.size(1))  # tensor.size(1) 是列数\n",
    "        # 输出最大的 1个元素的坐标\n",
    "        coordinates = torch.stack((rows, cols), dim=1)  #each_count中的位置坐标\n",
    "        print(coordinates )     #a=tensor([[0, 0]])\n",
    "        # int(a[0][1]) =0        \n",
    "        #############################找到点数最多的1个区域\n",
    "        # each_count=torch.zeros([len(idxs),3])\n",
    "        # for i in range(len(idxs)):\n",
    "        #     for j in range(3):\n",
    "        #             each_count[i,j]=data[\"image_{}\".format(i)][\"count{}\".format(j)]\n",
    "        # flattened_tensor = each_count.flatten()\n",
    "        # # 2. 获取最大的 1 个元素的索引\n",
    "        # values, indices = torch.topk(flattened_tensor, 1, largest=True)\n",
    "        # # 3. 将一维索引转换为二维坐标\n",
    "        # # 使用 divmod 来获取行和列\n",
    "        # rows, cols = np.divmod(indices, each_count.size(1))  # tensor.size(1) 是列数\n",
    "        # # 输出最大的 3个元素的坐标\n",
    "        # coordinates = torch.stack((rows, cols), dim=1)  #each_count中的位置坐标\n",
    "        # print(coordinates )     #a=tensor([[0, 0]])\n",
    "        # # int(a[0][1]) =0\n",
    "        ###############################找到点数最多的区域  -》标注\n",
    "        num_image_pick=int(coordinates[0][0])\n",
    "        num_area_pick=int(coordinates[0][1])\n",
    "        flag[ num_image_pick]=0\n",
    "        left=data[\"image_{}\".format( num_image_pick)][\"area{}\".format(num_area_pick)][0]   \n",
    "        right=data[\"image_{}\".format( num_image_pick)][\"area{}\".format(num_area_pick)][1]   \n",
    "        print(left,right)  #397,514\n",
    "        img_area=x.squeeze(1)[num_image_pick][:,left:right]\n",
    "        mask_area=y.squeeze(1)[num_image_pick][:,left:right]\n",
    "    ###################################对选定的区域进行可视化\n",
    "    # # 克隆图像\n",
    "        resultImg =predict[num_image_pick,:,:].cpu().numpy().copy()*255\n",
    "        resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "        # 创建一个彩色图像\n",
    "        m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        #绘制所选中区域\n",
    "        a=np.where( predict[num_image_pick,:,left:right]!=1)\n",
    "        b=(a[1]+int(left),a[0])\n",
    "        for i in range(b[0].size):\n",
    "            d=(b[0][i],b[1][i])\n",
    "            cv2.circle(m_resultImg, d, 1, (160,160,160), 1)\n",
    "\n",
    "        # 绘制三个区域点\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point0\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (0, 0, 255), 15)  # 红色圆圈\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point1\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (0, 255, 0), 15)  # 绿色圆圈\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point2\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (255, 255, 0), 15)  # 黄色圆圈\n",
    "\n",
    "        \n",
    "        # 使用matplotlib显示图像\n",
    "        # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "        m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 显示图像\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(m_resultImg_rgb)\n",
    "        # plt.axis('off')  # 不显示坐标轴\n",
    "        # plt.show()\n",
    "        plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/picture{}_pickarea_pridect_points.png\".format(seed,otherchoice,\"EntropySampling\",n,num_image_pick))\n",
    "        plt.close()\n",
    "\n",
    "        ###############################求这个区域的连通性\n",
    "        con_nums=[]\n",
    "        mask_area=mask_area.cpu().numpy().astype(np.uint8)\n",
    "        # 连通性分析\n",
    "        num_labels, labels = cv2.connectedComponents(mask_area, connectivity=8)\n",
    "        output_image = np.zeros((labels.shape[0], labels.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # 为每个连通组件指定不同的颜色\n",
    "        for label in range(1, num_labels):  # 0 是背景，跳过\n",
    "            con_nums.append(label)\n",
    "            output_image[labels == label] = np.random.randint(0, 255, 3)\n",
    "\n",
    "        # 显示图像\n",
    "        plt.imshow(output_image)\n",
    "        plt.axis('off')  # 不显示坐标轴\n",
    "        plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/area{}_connect.png\".format(seed,otherchoice,\"EntropySampling\",n,num_area_pick))\n",
    "        \n",
    "        np.savetxt('./active_learning_data/{}_{}/{}/pick/split_{}/area{}_labels.txt'.format(seed,otherchoice,\"EntropySampling\",n,num_area_pick), labels, fmt='%d', delimiter=',')\n",
    "        ######labels是一个矩阵，由0，1，2，3，，，，，类\n",
    "        #########################根据连通性切割小图，\n",
    "        new_train_imgs_small=torch.zeros([100,224,224])\n",
    "        new_train_masks_small=torch.zeros([100,224,224])\n",
    "        id=0\n",
    "        #################按照断层连通性\n",
    "        for label in range(1, num_labels):\n",
    "            fids,sids=np.where(labels==label)\n",
    "            # for ids in range(len(fids)):\n",
    "            #     maskContour.append((sids[ids],fids[ids]))\n",
    "            l=abs(fids[-1]-fids[0])\n",
    "            w=abs(sids[-1]-sids[0])\n",
    "            pickimg=np.zeros([l,w])\n",
    "            pickmask=np.zeros([l,w])\n",
    "            pickimg=img_area[min(fids[0],fids[-1]):max(fids[0],fids[-1]),min(sids[0],sids[-1]):max(sids[0],sids[-1])]\n",
    "            pickmask=mask_area[min(fids[0],fids[-1]):max(fids[0],fids[-1]),min(sids[0],sids[-1]):max(sids[0],sids[-1])]\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(pickimg.cpu())\n",
    "            \n",
    "            plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/{}_img.png\".format(seed,otherchoice,\"EntropySampling\",n,label))\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(pickmask)\n",
    "        \n",
    "            plt.savefig(\"./active_learning_data/{}_{}/{}/pick/split_{}/{}_mask.png\".format(seed,otherchoice,\"EntropySampling\",n,label))\n",
    "        \n",
    "            \n",
    "            a=fids[0]\n",
    "            b=sids[0]\n",
    "            \n",
    "            if (sids[-1]-sids[0] )>0:\n",
    "                while(1):\n",
    "                    # print(a,b)\n",
    "                    if a>288 or b>(right-left-224):\n",
    "                        break\n",
    "                    new_train_imgs_small[id]=torch.tensor(img_area[a:a+224,b:b+224])\n",
    "                    new_train_masks_small[id]=torch.tensor(mask_area[a:a+224,b:b+224])\n",
    "                \n",
    "                    c=np.where(fids==a+56)\n",
    "                    if c[0].size==0:\n",
    "                        break\n",
    "                    else:\n",
    "                        a=fids[c[0][0]]\n",
    "                        b=sids[c[0][0]]\n",
    "                        id+=1\n",
    "                        # print(index)\n",
    "                   \n",
    "\n",
    "            else:\n",
    "                while(1):\n",
    "                    print(a,b)\n",
    "                    if a>288 or b<224:\n",
    "                        break\n",
    "                    new_train_imgs_small[id]=torch.tensor(img_area[a:a+224,b-224:b])\n",
    "                    new_train_masks_small[id]=torch.tensor(mask_area[a:a+224,b-224:b])\n",
    "                    \n",
    "                    c=np.where(fids==a+56)\n",
    "                    if c[0].size==0:\n",
    "                        break\n",
    "                    else:\n",
    "                        a=fids[c[0][0]]\n",
    "                        b=sids[c[0][0]]\n",
    "                        id+=1\n",
    "        print(id)\n",
    "        \n",
    "        #################按照patch\n",
    "        # for j in range(4):\n",
    "            # for k in range(int(right-left)//128):\n",
    "            #     img1=img_area[j*128:(j+1)*128,k*128:(k+1)*128]\n",
    "            #     mask1=mask_area[j*128:(j+1)*128,k*128:(k+1)*128]\n",
    "            #     if mask1.sum()!=0:\n",
    "            #         new_train_imgs_small[id]=torch.tensor(img1)\n",
    "            #         new_train_masks_small[id]=torch.tensor(mask1)\n",
    "            #         id+=1\n",
    "        # print(id)\n",
    "        \n",
    "\n",
    "        new_train_imgs_small=new_train_imgs_small[:id]\n",
    "        new_train_masks_small=new_train_masks_small[:id]\n",
    "        if n==1:\n",
    "                new_train_imgs= new_train_imgs_small\n",
    "                new_train_masks=new_train_masks_small\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "        else:\n",
    "                imgbefor=np.load(\"./data/liuyue/active_learning/data/THEBE_224/train_img_small.npy\")\n",
    "                maskbefor=np.load(\"./data/liuyue/active_learning/data/THEBE_224/train_mask_small.npy\")\n",
    "                print(\"imgbefor:{}\".format(imgbefor.shape))\n",
    "                print(\"maskbefor:{}\".format(maskbefor.shape))\n",
    "                imgbefor=torch.tensor(imgbefor)\n",
    "                maskbefor=torch.tensor(maskbefor)\n",
    "                new_train_imgs_small=torch.tensor(new_train_imgs_small)\n",
    "                new_train_masks_small=torch.tensor(new_train_masks_small)\n",
    "                new_train_imgs= torch.cat((new_train_imgs_small, imgbefor), dim=0)\n",
    "                new_train_masks=torch.cat((new_train_masks_small, maskbefor), dim=0)\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "        \n",
    "        \n",
    "    \n",
    "        np.save(\"./data/liuyue/active_learning/data/THEBE_224/train_img_small.npy\",new_train_imgs)\n",
    "        np.save(\"./data/liuyue/active_learning/data/THEBE_224/train_mask_small.npy\",new_train_masks)              \n",
    "        print(flag)\n",
    "        return data,flag\n",
    "\n",
    "        \n",
    "\n",
    "##########################################\n",
    "    def predict_prob_LeastConfidence(self, data,n,seed,otherchoice,picknum,picknum_no,flag):\n",
    "        # self.clf = self.net().to(self.device)\n",
    "        vit_name=\"R50-ViT-B_16\"\n",
    "        img_size=224\n",
    "        vit_patches_size=16\n",
    "        config_vit = CONFIGS[vit_name]\n",
    "        if vit_name.find('R50') != -1:\n",
    "            config_vit.patches.grid = (\n",
    "            int(img_size / vit_patches_size), int(img_size / vit_patches_size))\n",
    "        self.clf=VisionTransformer(config_vit).to(self.device)\n",
    "        model_nestunet_path = \"./data/liuyue/active_learning_data/{}_{}/{}/SSL_checkpoint_best.pkl\".format(seed,otherchoice,\"LeastConfidence\")\n",
    "        weights = torch.load(model_nestunet_path, map_location=\"cuda\")['model_state_dict']\n",
    "        weights_dict = {}\n",
    "        for k, v in weights.items():\n",
    "                new_k = k.replace('module.', '') if 'module' in k else k\n",
    "                weights_dict[new_k] = v\n",
    "        self.clf.load_state_dict(weights_dict)\n",
    "\n",
    "        self.clf.eval()\n",
    "        # prob=[]\n",
    "        \n",
    "        \n",
    "        loader = DataLoader(data, shuffle=False, **self.params['trainsmall_args'])\n",
    "        with torch.no_grad():\n",
    "            for x, y, idxs in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                outputs=np.zeros([len(idxs),2,512,2048])\n",
    "        for idx in idxs:\n",
    "            recover_Y_test_pred=predict_slice(THEBE_Net, x[idx].squeeze().cpu(),\"LeastConfidence\",seed,otherchoice)#512,2048,1\n",
    "            outputs[idx,1,:,:]=np.squeeze(recover_Y_test_pred)\n",
    "\n",
    "        outputs[:,0,:,:]=1-outputs[:,1,:,:]\n",
    "        outputs=torch.tensor(outputs)\n",
    "        predict= torch.argmax(outputs,dim=1)   \n",
    "        num=1-torch.max(outputs[:,0,:,:],outputs[:,1,:,:])\n",
    "        num=num.cpu()\n",
    "\n",
    "\n",
    "        data={}\n",
    "        for idx in idxs:\n",
    "            if flag[idx]==1:\n",
    "                points=max_50(num[idx])  #50个坐标\n",
    "                # print(points)\n",
    "                labels, centroids=kmeans(points)   #labels=0,1,2\n",
    "                ####################################可视化 聚类的点\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.scatter(points[:, 1], points[:, 0],s=10, c=labels, cmap='viridis')\n",
    "                plt.scatter(centroids[:, 1], centroids[:, 0], s=20, c='red', marker='X')  # 绘制簇中心\n",
    "                plt.title(\"K-means Clustering (K=3)\")\n",
    "                plt.xlabel(\"X\")\n",
    "                plt.ylabel(\"Y\")\n",
    "                plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/picture{}_kmeans.png\".format(seed,otherchoice,\"LeastConfidence\",n,idx))\n",
    "                plt.close()\n",
    "\n",
    "               \n",
    "                #############################创建字典\n",
    "                labels0=np.where(labels==0)   #索引值\n",
    "                labels1=np.where(labels==1)\n",
    "                labels2=np.where(labels==2)\n",
    "                point0=points[labels0]  #对应的区域点\n",
    "                point1=points[labels1]\n",
    "                point2=points[labels2]\n",
    "                area0=(point0[:,1].min(),point0[:,1].max())\n",
    "                area1=(point1[:,1].min(),point1[:,1].max())\n",
    "                area2=(point2[:,1].min(),point2[:,1].max())\n",
    "                count0=len(labels0[0])\n",
    "                count1=len(labels1[0])\n",
    "                count2=len(labels2[0])\n",
    "\n",
    "                data[\"image_{}\".format(idx)]={\"area0\": area0 ,\"point0\": point0 ,\"count0\":0,\"area1\":  area1,\"point1\":  point1 ,\"count1\":0,\"area2\": area2  ,\"point2\": point2,\"count2\":0}                        \n",
    "                # print(data) \n",
    "\n",
    "                ####################################可视化 pridect\n",
    "                # # 克隆图像\n",
    "                resultImg =predict[idx,:,:].cpu().numpy().copy()*255\n",
    "                resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "                # 创建一个彩色图像\n",
    "                m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                # 遍历每个连通域点并绘制红色圆圈\n",
    "                for point in point0:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (0, 0, 255), 15)  # 红色圆圈\n",
    "                for point in point1:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (0, 255, 0), 15)  # 红色圆圈\n",
    "                for point in point2:\n",
    "                    point=tuple(point.tolist())\n",
    "                    point_change=(point[1],point[0])\n",
    "                    cv2.circle(m_resultImg, point_change, 1, (255, 255, 0), 15)  # 红色圆圈\n",
    "\n",
    "                \n",
    "                # 使用matplotlib显示图像\n",
    "                # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "                m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # 显示图像\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(m_resultImg_rgb)\n",
    "                # plt.axis('off')  # 不显示坐标轴\n",
    "                # plt.show()\n",
    "                plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/picture{}_pridect_points.png\".format(seed,otherchoice,\"LeastConfidence\",n,idx))\n",
    "                plt.close()\n",
    "                \n",
    "            else:\n",
    "                    data[\"image_{}\".format(idx)]={\"area0\":[] ,\"point0\": [] ,\"count0\":0,\"area1\":  [],\"point1\":  [] ,\"count1\":0,\"area2\": []  ,\"point2\": [],\"count2\":0}                        \n",
    "               \n",
    "                    continue\n",
    "        # print(data)     \n",
    "        # with open('/home/user/data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/data.txt'.format(seed,otherchoice,\"LeastConfidence\",n), 'w') as f:\n",
    "        #     json.dump(data, f)\n",
    "        # ########################计算区域内标注的和\n",
    "        area_sum=torch.zeros([len(idxs),3])\n",
    "        for i in range(len(idxs)):\n",
    "            for j in range(3):\n",
    "                if flag[i]==1:\n",
    "                    left=data[\"image_{}\".format(i)][\"area{}\".format(j)][0]\n",
    "                    right=data[\"image_{}\".format(i)][\"area{}\".format(j)][1]\n",
    "                    sum=torch.sum(predict[i,:,left:right])\n",
    "                    area_sum[i,j]=sum\n",
    "                    data[\"image_{}\".format(i)][\"count{}\".format(j)]=sum\n",
    "\n",
    "\n",
    "        #############################找到和最大的1个区域\n",
    "        flattened_tensor = area_sum.flatten()\n",
    "        # 2. 获取最大的 1 个元素的索引\n",
    "        values, indices = torch.topk(flattened_tensor, 1, largest=True)\n",
    "        # 3. 将一维索引转换为二维坐标\n",
    "        # 使用 divmod 来获取行和列\n",
    "        rows, cols = np.divmod(indices, area_sum.size(1))  # tensor.size(1) 是列数\n",
    "        # 输出最大的 1个元素的坐标\n",
    "        coordinates = torch.stack((rows, cols), dim=1)  #each_count中的位置坐标\n",
    "        print(coordinates )     #a=tensor([[0, 0]])\n",
    "        # int(a[0][1]) =0\n",
    "        #############################找到点数最多的1个区域\n",
    "        # each_count=torch.zeros([len(idxs),3])\n",
    "        # for i in range(len(idxs)):\n",
    "        #     for j in range(3):\n",
    "        #             each_count[i,j]=data[\"image_{}\".format(i)][\"count{}\".format(j)]\n",
    "        # flattened_tensor = each_count.flatten()\n",
    "        # # 2. 获取最大的 1 个元素的索引\n",
    "        # values, indices = torch.topk(flattened_tensor, 1, largest=True)\n",
    "        # # 3. 将一维索引转换为二维坐标\n",
    "        # # 使用 divmod 来获取行和列\n",
    "        # rows, cols = np.divmod(indices, each_count.size(1))  # tensor.size(1) 是列数\n",
    "        # # 输出最大的 3个元素的坐标\n",
    "        # coordinates = torch.stack((rows, cols), dim=1)  #each_count中的位置坐标\n",
    "        # print(coordinates )     #a=tensor([[0, 0]])\n",
    "        # # int(a[0][1]) =0\n",
    "        ###############################找到点数最多的区域  -》标注\n",
    "        num_image_pick=int(coordinates[0][0])\n",
    "        num_area_pick=int(coordinates[0][1])\n",
    "        flag[ num_image_pick]=0\n",
    "        left=data[\"image_{}\".format( num_image_pick)][\"area{}\".format(num_area_pick)][0]   \n",
    "        right=data[\"image_{}\".format( num_image_pick)][\"area{}\".format(num_area_pick)][1]   \n",
    "        print(left,right)  #397,514\n",
    "        img_area=x.squeeze(1)[num_image_pick][:,left:right]\n",
    "        mask_area=y.squeeze(1)[num_image_pick][:,left:right]\n",
    "    ###################################对选定的区域进行可视化\n",
    "    # # 克隆图像\n",
    "        resultImg =predict[num_image_pick,:,:].cpu().numpy().copy()*255\n",
    "        resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "        # 创建一个彩色图像\n",
    "        m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        #绘制所选中区域\n",
    "        a=np.where( predict[num_image_pick,:,left:right]!=1)\n",
    "        b=(a[1]+int(left),a[0])\n",
    "        for i in range(b[0].size):\n",
    "            d=(b[0][i],b[1][i])\n",
    "            cv2.circle(m_resultImg, d, 1, (160,160,160), 1)\n",
    "\n",
    "        # 绘制三个区域点\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point0\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (0, 0, 255), 15)  # 红色圆圈\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point1\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (0, 255, 0), 15)  # 绿色圆圈\n",
    "        for point in data[\"image_{}\".format( num_image_pick)][\"point2\"]:\n",
    "            point=tuple(point.tolist())\n",
    "            point_change=(point[1],point[0])\n",
    "            cv2.circle(m_resultImg, point_change, 1, (255, 255, 0), 15)  # 黄色圆圈\n",
    "\n",
    "        \n",
    "        # 使用matplotlib显示图像\n",
    "        # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "        m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 显示图像\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(m_resultImg_rgb)\n",
    "        # plt.axis('off')  # 不显示坐标轴\n",
    "        # plt.show()\n",
    "        plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/picture{}_pickarea_pridect_points.png\".format(seed,otherchoice,\"LeastConfidence\",n,num_image_pick))\n",
    "        plt.close()\n",
    "\n",
    "        ###############################求这个区域的连通性\n",
    "        con_nums=[]\n",
    "        mask_area=mask_area.cpu().numpy().astype(np.uint8)\n",
    "        # 连通性分析\n",
    "        num_labels, labels = cv2.connectedComponents(mask_area, connectivity=8)\n",
    "        output_image = np.zeros((labels.shape[0], labels.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # 为每个连通组件指定不同的颜色\n",
    "        for label in range(1, num_labels):  # 0 是背景，跳过\n",
    "            con_nums.append(label)\n",
    "            output_image[labels == label] = np.random.randint(0, 255, 3)\n",
    "\n",
    "        # 显示图像\n",
    "        plt.imshow(output_image)\n",
    "        plt.axis('off')  # 不显示坐标轴\n",
    "        plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/area{}_connect.png\".format(seed,otherchoice,\"LeastConfidence\",n,num_area_pick))\n",
    "        \n",
    "        np.savetxt('./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/area{}_labels.txt'.format(seed,otherchoice,\"LeastConfidence\",n,num_area_pick), labels, fmt='%d', delimiter=',')\n",
    "        ######labels是一个矩阵，由0，1，2，3，，，，，类\n",
    "        #########################根据连通性切割小图，\n",
    "        new_train_imgs_small=torch.zeros([100,224,224])\n",
    "        new_train_masks_small=torch.zeros([100,224,224])\n",
    "        id=0\n",
    "        #################按照断层连通性\n",
    "        for label in range(1, num_labels):\n",
    "            fids,sids=np.where(labels==label)\n",
    "            # for ids in range(len(fids)):\n",
    "            #     maskContour.append((sids[ids],fids[ids]))\n",
    "            l=abs(fids[-1]-fids[0])\n",
    "            w=abs(sids[-1]-sids[0])\n",
    "            pickimg=np.zeros([l,w])\n",
    "            pickmask=np.zeros([l,w])\n",
    "            pickimg=img_area[min(fids[0],fids[-1]):max(fids[0],fids[-1]),min(sids[0],sids[-1]):max(sids[0],sids[-1])]\n",
    "            pickmask=mask_area[min(fids[0],fids[-1]):max(fids[0],fids[-1]),min(sids[0],sids[-1]):max(sids[0],sids[-1])]\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(pickimg.cpu())\n",
    "            \n",
    "            plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/{}_img.png\".format(seed,otherchoice,\"LeastConfidence\",n,label))\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(pickmask)\n",
    "        \n",
    "            plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/{}_mask.png\".format(seed,otherchoice,\"LeastConfidence\",n,label))\n",
    "        \n",
    "            \n",
    "            a=fids[0]\n",
    "            b=sids[0]\n",
    "            \n",
    "            if (sids[-1]-sids[0] )>0:\n",
    "                while(1):\n",
    "                    # print(a,b)\n",
    "                    if a>288 or b>(right-left-224):\n",
    "                        break\n",
    "                    new_train_imgs_small[id]=torch.tensor(img_area[a:a+224,b:b+224])\n",
    "                    new_train_masks_small[id]=torch.tensor(mask_area[a:a+224,b:b+224])\n",
    "                \n",
    "                    c=np.where(fids==a+56)\n",
    "                    if c[0].size==0:\n",
    "                        break\n",
    "                    else:\n",
    "                        a=fids[c[0][0]]\n",
    "                        b=sids[c[0][0]]\n",
    "                        id+=1\n",
    "                        # print(index)\n",
    "                   \n",
    "\n",
    "            else:\n",
    "                while(1):\n",
    "                    print(a,b)\n",
    "                    if a>288 or b<224:\n",
    "                        break\n",
    "                    new_train_imgs_small[id]=torch.tensor(img_area[a:a+224,b-224:b])\n",
    "                    new_train_masks_small[id]=torch.tensor(mask_area[a:a+224,b-224:b])\n",
    "                    \n",
    "                    c=np.where(fids==a+56)\n",
    "                    if c[0].size==0:\n",
    "                        break\n",
    "                    else:\n",
    "                        a=fids[c[0][0]]\n",
    "                        b=sids[c[0][0]]\n",
    "                        id+=1\n",
    "        # print(id)\n",
    "        \n",
    "        #################按照patch\n",
    "        # for j in range(4):\n",
    "            # for k in range(int(right-left)//128):\n",
    "            #     img1=img_area[j*128:(j+1)*128,k*128:(k+1)*128]\n",
    "            #     mask1=mask_area[j*128:(j+1)*128,k*128:(k+1)*128]\n",
    "            #     if mask1.sum()!=0:\n",
    "            #         new_train_imgs_small[id]=torch.tensor(img1)\n",
    "            #         new_train_masks_small[id]=torch.tensor(mask1)\n",
    "            #         id+=1\n",
    "        # print(id)\n",
    "        \n",
    "\n",
    "        new_train_imgs_small=new_train_imgs_small[:id]\n",
    "        new_train_masks_small=new_train_masks_small[:id]\n",
    "        if n==1:\n",
    "                new_train_imgs= new_train_imgs_small\n",
    "                new_train_masks=new_train_masks_small\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "        else:\n",
    "                imgbefor=np.load(\"./data/liuyue/active_learning/data/THEBE_224/train_img_small.npy\")\n",
    "                maskbefor=np.load(\"./data/liuyue/active_learning/data/THEBE_224/train_mask_small.npy\")\n",
    "                print(\"imgbefor:{}\".format(imgbefor.shape))\n",
    "                print(\"maskbefor:{}\".format(maskbefor.shape))\n",
    "                imgbefor=torch.tensor(imgbefor)\n",
    "                maskbefor=torch.tensor(maskbefor)\n",
    "                new_train_imgs_small=torch.tensor(new_train_imgs_small)\n",
    "                new_train_masks_small=torch.tensor(new_train_masks_small)\n",
    "                new_train_imgs= torch.cat((new_train_imgs_small, imgbefor), dim=0)\n",
    "                new_train_masks=torch.cat((new_train_masks_small, maskbefor), dim=0)\n",
    "                print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "        \n",
    "        \n",
    "    \n",
    "        np.save(\"./data/liuyue/active_learning/data/THEBE_224/train_img_small.npy\",new_train_imgs)\n",
    "        np.save(\"./data/liuyue/active_learning/data/THEBE_224/train_mask_small.npy\",new_train_masks)              \n",
    "        print(flag)\n",
    "        return data,flag\n",
    "        \n",
    "\n",
    "    def predict_prob_BALD_dropout(self, data, n_drop, n, seed,otherchoice,picknum,picknum_no):\n",
    "            self.clf = self.net().to(self.device)\n",
    "            model_nestunet_path = \"./data/liuyue/active_learning_data/{}_{}/{}/SSL_checkpoint_best.pkl\".format(seed,otherchoice,\"BALDDropout\")\n",
    "            weights = torch.load(model_nestunet_path, map_location=\"cuda\")['model_state_dict']\n",
    "            weights_dict = {}\n",
    "            for k, v in weights.items():\n",
    "                    new_k = k.replace('module.', '') if 'module' in k else k\n",
    "                    weights_dict[new_k] = v\n",
    "            self.clf.load_state_dict(weights_dict)\n",
    "\n",
    "            \n",
    "            self.clf.train()\n",
    "            probs = torch.zeros([picknum,7])  \n",
    "            probs_no=[]\n",
    "            num= torch.zeros([n_drop,2,512,2048])\n",
    "            loader = DataLoader(data, shuffle=False, **self.params['trainsmall_args'])\n",
    "            maskContour=[]\n",
    "            maskcount=[]\n",
    "            maskcount1=[]\n",
    "            for nd in range(n_drop):\n",
    "                with torch.no_grad():\n",
    "                    for x, y, idxs in loader:\n",
    "                        x, y = x.to(self.device), y.to(self.device)\n",
    "                        x_select=x[n-1].unsqueeze(0) #1,512,2048\n",
    "                        \n",
    "                        y_selsct=y[n-1].unsqueeze(0)  #1,1,512,2048\n",
    "                        mask0=y.squeeze(1).cpu()\n",
    "                        out  = self.clf(x_select)\n",
    "                        \n",
    "                        outputs=torch.softmax(out, dim=1)\n",
    "                        out1=outputs[0,1,:,:].detach().cpu().numpy()\n",
    "                        pred_resnetunet_vision = cv2.applyColorMap((out1 * 255).astype(np.uint8),cmapy.cmap('jet_r'))\n",
    "                        plt.imshow(pred_resnetunet_vision)\n",
    "                        plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/nd{}_out.png\".format(seed,otherchoice,\"BALDDropout\",n,nd))\n",
    "                        \n",
    "                        \n",
    "                        predict= torch.argmax(outputs,dim=1)             \n",
    "                        img1=predict[0,:,:].cpu()\n",
    "                    \n",
    "                        plt.figure(figsize=(4,4))\n",
    "                        plt.imshow(img1,\"gray\")\n",
    "                        plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/nd{}_predict.png\".format(seed,otherchoice,\"BALDDropout\",n,nd))\n",
    "                    \n",
    "                        \n",
    "                        mask=y_selsct.squeeze(1)[0,:,:].cpu()\n",
    "                        plt.figure(figsize=(4,4))\n",
    "                        plt.imshow(mask,\"gray\")\n",
    "                        plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/nd{}_mask.png\".format(seed,otherchoice,\"BALDDropout\",n,nd))\n",
    "                \n",
    "                        \n",
    "                        num[nd,0]=outputs[0,0,:,:]\n",
    "                        num[nd,1]=outputs[0,1,:,:]\n",
    "\n",
    "            pb = num.mean(0)\n",
    "            entropy1 = (-pb*torch.log(pb)).sum(1)\n",
    "            entropy2 = (-num*torch.log(num)).sum(2).mean(0)\n",
    "            entr_sum = entropy2 - entropy1\n",
    "            entr_sum=entr_sum.cpu()\n",
    "            np.savetxt('./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/entr_sum.txt'.format(seed,otherchoice,\"BALDDropout\",n), entr_sum, fmt='%d', delimiter=',')\n",
    "            id=0\n",
    "            while(1):\n",
    "                num_min=entr_sum.min()\n",
    "                num_max=entr_sum.max()\n",
    "                fid=np.where(entr_sum==num_max)[0][0]\n",
    "                # print(fid)\n",
    "                \n",
    "                sid=np.where(entr_sum==num_max)[1][0]\n",
    "                if predict[0,fid,sid]==1:\n",
    "                    probs[id][0]=fid\n",
    "                    probs[id][1]=sid\n",
    "                    maskContour.append((sid,fid))\n",
    "                    probs[id][2]=num_max\n",
    "                    probs[id][3]=outputs[0,0,fid,sid]\n",
    "                    probs[id][4]=outputs[0,1,fid,sid]\n",
    "                    probs[id][5]=predict[0,fid,sid]\n",
    "                    probs[id][6]=mask0[0,fid,sid]\n",
    "                    id+=1\n",
    "                \n",
    "                else:\n",
    "                    probs_no.append((fid,sid))\n",
    "                    maskContour.append((sid,fid))\n",
    "                entr_sum[fid][sid]=num_min-1\n",
    "\n",
    "                if id==picknum:\n",
    "                    break\n",
    "            if len(probs_no)  <picknum_no:\n",
    "                while(1):\n",
    "                    num_min=entr_sum.min()\n",
    "                    num_max=entr_sum.max()\n",
    "                    fid=np.where(entr_sum==num_max)[0][0]\n",
    "                    # print(fid)\n",
    "                    \n",
    "                    sid=np.where(entr_sum==num_max)[1][0]\n",
    "                    probs_no.append((fid,sid)) \n",
    "                    maskcount1.append((sid,fid))\n",
    "                    entr_sum[fid][sid]=num_min-1\n",
    "                    if len(probs_no)  ==picknum_no:\n",
    "                            break\n",
    "                        \n",
    "\n",
    "            \n",
    "            # 克隆图像\n",
    "            resultImg = mask.numpy().copy()*255\n",
    "            resultImg= np.uint8(np.clip(resultImg, 0, 255))  # 限制范围在 [0, 255] 之间\n",
    "\n",
    "            # 创建一个彩色图像\n",
    "            m_resultImg = cv2.cvtColor(resultImg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # 遍历每个连通域点并绘制红色圆圈\n",
    "            for point in maskContour:\n",
    "                cv2.circle(m_resultImg, point, 1, (0, 0, 255), 10)  # 红色圆圈\n",
    "\n",
    "            for point in maskcount1:\n",
    "                cv2.circle(m_resultImg, point, 1, (0, 255, 225), 10)\n",
    "\n",
    "            # 使用matplotlib显示图像\n",
    "            # matplotlib默认是RGB格式，所以要将BGR格式转换为RGB\n",
    "            m_resultImg_rgb = cv2.cvtColor(m_resultImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # 显示图像\n",
    "            plt.imshow(m_resultImg_rgb)\n",
    "            # plt.axis('off')  # 不显示坐标轴\n",
    "            # plt.show()\n",
    "            plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/_mask_points.png\".format(seed,otherchoice,\"BALDDropout\",n))\n",
    "\n",
    "\n",
    "\n",
    "            new_train_imgs_small=np.zeros([picknum+picknum_no,128,128])\n",
    "            new_train_masks_small=np.zeros([picknum+picknum_no,128,128])\n",
    "            pick_train_imgs=np.zeros([picknum,128,128])\n",
    "            pick_train_masks=np.zeros([picknum,128,128])\n",
    "            image=x_select.squeeze().cpu()\n",
    "            masks=y_selsct.squeeze().cpu()     \n",
    "            bigimage=np.zeros([640,2176])\n",
    "            bigmask=np.zeros([640,2176])\n",
    "            for i in range(64,576):\n",
    "                for j in range(64,2112):\n",
    "                    bigimage[i][j]=image[i-64][j-64]\n",
    "                    bigmask[i][j]=masks[i-64][j-64]\n",
    "            for i in range(picknum):\n",
    "                firstid=probs[i][0]+64\n",
    "                secondid=probs[i][1]+64\n",
    "                for j in range(128):\n",
    "                    for z in range(128):\n",
    "                        pick_train_imgs[i][j][z]=bigimage[int(firstid-64+j)][int(secondid-64+z)]\n",
    "                        pick_train_masks[i][j][z]=bigmask[int(firstid-64+j)][int(secondid-64+z)]\n",
    "                pick_train_masks[i][64][64]=5\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(pick_train_imgs[i])\n",
    "                plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/{}_img.png\".format(seed,otherchoice,\"BALDDropout\",n,i))\n",
    "\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(pick_train_masks[i])\n",
    "                plt.savefig(\"./data/liuyue/active_learning_data/{}_{}/{}/pick/split_{}/{}_mask.png\".format(seed,otherchoice,\"BALDDropout\",n,i))\n",
    "\n",
    "            for i in range(picknum):\n",
    "                firstid=probs[i][0]\n",
    "                secondid=probs[i][1]\n",
    "                if firstid<64:\n",
    "                    firstid=64\n",
    "                if firstid>448:\n",
    "                    firstid=448\n",
    "                if secondid<64:\n",
    "                    secondid=64\n",
    "                if secondid>1984:\n",
    "                    secondid=1984\n",
    "                for j in range(128):\n",
    "                    for z in range(128):\n",
    "                        new_train_imgs_small[i][j][z]=image[int(firstid-64+j)][int(secondid-64+z)]\n",
    "                        new_train_masks_small[i][j][z]=masks[int(firstid-64+j)][int(secondid-64+z)]\n",
    "            \n",
    "                for i in range(picknum_no):\n",
    "                    firstid,secondid=probs_no[i]\n",
    "                    \n",
    "                    if firstid<64:\n",
    "                        firstid=64\n",
    "                    if firstid>448:\n",
    "                        firstid=448\n",
    "                    if secondid<64:\n",
    "                        secondid=64\n",
    "                    if secondid>1984:\n",
    "                        secondid=1984\n",
    "                    for j in range(128):\n",
    "                        for z in range(128):\n",
    "                            new_train_imgs_small[i+picknum][j][z]=image[int(firstid-64+j)][int(secondid-64+z)]\n",
    "                            new_train_masks_small[i+picknum][j][z]=masks[int(firstid-64+j)][int(secondid-64+z)]\n",
    "\n",
    "\n",
    "            if n==1:\n",
    "                    new_train_imgs= new_train_imgs_small\n",
    "                    new_train_masks=new_train_masks_small\n",
    "                    print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                    print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "            else:\n",
    "                    imgbefor=np.load(\"./data/liuyue/active_learning/data/THEBE_NEW/train_img_small.npy\")\n",
    "                    maskbefor=np.load(\"./data/liuyue/active_learning/data/THEBE_NEW/train_mask_small.npy\")\n",
    "                    print(\"imgbefor:{}\".format(imgbefor.shape))\n",
    "                    print(\"maskbefor:{}\".format(maskbefor.shape))\n",
    "                    imgbefor=torch.tensor(imgbefor)\n",
    "                    maskbefor=torch.tensor(maskbefor)\n",
    "                    new_train_imgs_small=torch.tensor(new_train_imgs_small)\n",
    "                    new_train_masks_small=torch.tensor(new_train_masks_small)\n",
    "                    new_train_imgs= torch.cat((new_train_imgs_small, imgbefor), dim=0)\n",
    "                    new_train_masks=torch.cat((new_train_masks_small, maskbefor), dim=0)\n",
    "                    print(\"new_train_imgs:{}\".format(new_train_imgs.shape))\n",
    "                    print(\"new_train_masks:{}\".format(new_train_masks.shape))\n",
    "            \n",
    "            \n",
    "        \n",
    "            np.save(\"./data/liuyue/active_learning/data/THEBE_NEW/train_img_small.npy\",new_train_imgs)\n",
    "            np.save(\"./data/liuyue/active_learning/data/THEBE_NEW/train_mask_small.npy\",new_train_masks)\n",
    "                        \n",
    "                        \n",
    "            return probs\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor,smooth):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "\n",
    "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "\n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum((1, 2))  # Will be zzero if both are 0\n",
    "    iou = (intersection + smooth) / (union + smooth)  # We smooth our devision to avoid 0/0\n",
    "    return iou\n",
    "\n",
    "class faultsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,preprocessed_images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.images = preprocessed_images\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = TF.to_tensor(image)\n",
    "        image=norm(image)\n",
    "        image = TF.normalize(image, [4.0902375e-05, ], [0.0383472, ])\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "        self.bce_func =  nn.BCELoss()\n",
    "        # self.dice_func = BinaryDiceLoss()#使用binarydiceloss\n",
    "        # self.dice_func=soft_cldice_loss()#使用soft_cldice_loss\n",
    "\n",
    "    # loss = loss_f(outputs_1.cpu(), outputs.cpu(), labels.cpu())\n",
    "    def forward(self, predict, target):\n",
    "        loss_bce=self.bce_func(predict,target)\n",
    "        # loss_dice=self.dice_func(predict,target)\n",
    "        # return 0.5*loss_dice + 0.5*loss_bce\n",
    "        return loss_bce\n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def min_50(tensor):\n",
    "        #  # 创建一个随机的 tensor 矩阵（假设为二维矩阵）\n",
    "        # tensor = torch.randn(100, 100)  # 例如，一个 10x10 的矩阵\n",
    "\n",
    "        # 1. 将 Tensor 展开为一维\n",
    "        flattened_tensor = tensor.flatten()\n",
    "\n",
    "        # 2. 获取最小的50 个元素的索引\n",
    "        values, indices = torch.topk(flattened_tensor, 50, largest=False)\n",
    "\n",
    "        # 3. 将一维索引转换为二维坐标\n",
    "        # 使用 divmod 来获取行和列\n",
    "        rows, cols = np.divmod(indices, tensor.size(1))  # tensor.size(1) 是列数\n",
    "\n",
    "        # 输出最小的 50 个元素的坐标\n",
    "        coordinates = torch.stack((rows, cols), dim=1)\n",
    "        # print(\"最小的 100 个像素点的坐标：\", coordinates)\n",
    "        return coordinates\n",
    "\n",
    "def max_50(tensor):\n",
    "        #  # 创建一个随机的 tensor 矩阵（假设为二维矩阵）\n",
    "        # tensor = torch.randn(100, 100)  # 例如，一个 10x10 的矩阵\n",
    "\n",
    "        # 1. 将 Tensor 展开为一维\n",
    "        flattened_tensor = tensor.flatten()\n",
    "\n",
    "        # 2. 获取最小的 100 个元素的索引\n",
    "        values, indices = torch.topk(flattened_tensor, 50, largest=True)\n",
    "\n",
    "        # 3. 将一维索引转换为二维坐标\n",
    "        # 使用 divmod 来获取行和列\n",
    "        rows, cols = np.divmod(indices, tensor.size(1))  # tensor.size(1) 是列数\n",
    "\n",
    "        # 输出最大的 50 个元素的坐标\n",
    "        coordinates = torch.stack((rows, cols), dim=1)\n",
    "        # print(\"最小的 100 个像素点的坐标：\", coordinates)\n",
    "        return coordinates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kmeans(coordinates ):\n",
    "    \n",
    "\n",
    "        # # 假设这50个坐标点是如下的随机数据\n",
    "        # coordinates = np.random.rand(50, 2)  # 50个二维坐标点，数据范围是[0,1]\n",
    "\n",
    "        # 使用 K-means 聚类\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)  # 设置为3类\n",
    "        kmeans.fit(coordinates)\n",
    "\n",
    "        # 获取每个点的分类标签\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        # 获取簇中心\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        return labels, centroids\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        e1 = F.relu(self.fc1(x))\n",
    "        x = F.dropout(e1, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x, e1\n",
    "\n",
    "    def get_embedding_dim(self):\n",
    "        return 50\n",
    "\n",
    "class SVHN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVHN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.conv3_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(1152, 400)\n",
    "        self.fc2 = nn.Linear(400, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
    "        x = x.view(-1, 1152)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        e1 = F.relu(self.fc2(x))\n",
    "        x = F.dropout(e1, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x, e1\n",
    "\n",
    "    def get_embedding_dim(self):\n",
    "        return 50\n",
    "\n",
    "class CIFAR10_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(1024, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 1024)\n",
    "        e1 = F.relu(self.fc1(x))\n",
    "        x = F.dropout(e1, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x, e1\n",
    "\n",
    "    def get_embedding_dim(self):\n",
    "        return 50\n",
    "    \n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "        super(DoubleConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "class Down(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__(\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.up(x1)\n",
    "        # [N, C, H, W]\n",
    "        diff_y = x2.size()[2] - x1.size()[2]\n",
    "        diff_x = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # padding_left, padding_right, padding_top, padding_bottom\n",
    "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,\n",
    "                        diff_y // 2, diff_y - diff_y // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(OutConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "class FAULTSEG_Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 num_classes: int = 2,###############################################\n",
    "                 bilinear: bool = True,\n",
    "                 base_c: int = 32):\n",
    "        super(FAULTSEG_Net, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.in_conv = DoubleConv(in_channels, base_c)\n",
    "        self.down1 = Down(base_c, base_c * 2)\n",
    "        self.down2 = Down(base_c * 2, base_c * 4)\n",
    "        self.down3 = Down(base_c * 4, base_c * 8)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(base_c * 8, base_c * 16 // factor)\n",
    "        self.up1 = Up(base_c * 16, base_c * 8 // factor, bilinear)\n",
    "        self.up2 = Up(base_c * 8, base_c * 4 // factor, bilinear)\n",
    "        self.up3 = Up(base_c * 4, base_c * 2 // factor, bilinear)\n",
    "        self.up4 = Up(base_c * 2, base_c, bilinear)\n",
    "        self.out_conv = OutConv(base_c, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        if x.size()[1] == 1 and self.in_channels == 3:  # 如果channel 是1，变成3\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        x1 = self.in_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.out_conv(x)\n",
    "        logits=torch.sigmoid(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "class PowerAvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride, p=2):\n",
    "        super(PowerAvgPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.p = p  # The power to which to raise each element before averaging\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply power (raise to the power of p) to the input\n",
    "        x = torch.pow(x, self.p)  # Raise each element to the power of p\n",
    "        # Apply average pooling\n",
    "        x = nn.functional.avg_pool2d(x, self.kernel_size, self.stride)\n",
    "        # Apply inverse power to return to the original scale\n",
    "        x = torch.pow(x, 1 / self.p)  # Inverse power to recover from the raised value\n",
    "        return x\n",
    "\n",
    "# self.pool = PowerAvgPool2d(kernel_size=2, stride=2, p=2)  # Using power of 2\n",
    "\n",
    "class THEBE_Net(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 num_classes: int = 2,###############################################\n",
    "                 bilinear: bool = True,\n",
    "                 base_c: int = 64):\n",
    "        super(THEBE_Net, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.in_conv = DoubleConv(in_channels, base_c)\n",
    "        self.down1 = Down(base_c, base_c * 2)\n",
    "        self.down2 = Down(base_c * 2, base_c * 4)\n",
    "        self.down3 = Down(base_c * 4, base_c * 8)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(base_c * 8, base_c * 16 // factor)\n",
    "        self.up1 = Up(base_c * 16, base_c * 8 // factor, bilinear)\n",
    "        self.up2 = Up(base_c * 8, base_c * 4 // factor, bilinear)\n",
    "        self.up3 = Up(base_c * 4, base_c * 2 // factor, bilinear)\n",
    "        self.up4 = Up(base_c * 2, base_c, bilinear)\n",
    "        self.out_conv = OutConv(base_c, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        if x.size()[1] == 1 and self.in_channels == 3:  # 如果channel 是1，变成3\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        x1 = self.in_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.out_conv(x)\n",
    "        # logits=torch.sigmoid(logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block with two 3x3 convolutions and ReLU activations.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3c2f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "from torchvision import transforms\n",
    "# from handlers import THEBE_Handler,FAULTSEG_Handler\n",
    "from data import get_THEBE,get_FAULTSEG\n",
    "\n",
    "# from nets_test_transunet import Net, MNIST_Net, SVHN_Net, CIFAR10_Net,THEBE_Net,FAULTSEG_Net\n",
    "from query_strategies import RandomSampling, LeastConfidence, MarginSampling, EntropySampling, \\\n",
    "                            BALDDropout \n",
    "                             \n",
    "\n",
    "params = {  'FAULTSEG':\n",
    "              {'n_epoch': 100, \n",
    "               'train_args':{'batch_size':16, 'num_workers': 4},\n",
    "               'val_args':{'batch_size': 8, 'num_workers': 4},\n",
    "               'test_args':{'batch_size': 4, 'num_workers': 4},#50\n",
    "               'optimizer_args':{'lr': 0.002, 'momentum': 0.9}},\n",
    "           'THEBE':\n",
    "              {'n_epoch': 50, \n",
    "               'train_args':{'batch_size':8, 'num_workers': 0},\n",
    "               'trainsmall_args':{'batch_size':2, 'num_workers': 0},\n",
    "               'val_args':{'batch_size': 8, 'num_workers': 0},\n",
    "               'test_args':{'batch_size':8 , 'num_workers': 0},#50\n",
    "               'optimizer_args':{'lr': 0.002, 'momentum': 0.9}}\n",
    "          }\n",
    "\n",
    "def get_handler(name):\n",
    "\n",
    "    if name == 'THEBE':\n",
    "        return THEBE_Handler\n",
    "    elif name == 'FAULTSEG':\n",
    "        return FAULTSEG_Handler\n",
    "    \n",
    "def get_dataset(name):\n",
    "    if name == 'THEBE':\n",
    "        return get_THEBE(get_handler(name))\n",
    "    elif name == 'FAULTSEG':\n",
    "        return get_FAULTSEG(get_handler(name))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "def get_net(name, device):\n",
    "    \n",
    "    if name == 'THEBE':\n",
    "        return Net(THEBE_Net, params[name], device)\n",
    "    elif name == 'FAULTSEG':\n",
    "        return Net(FAULTSEG_Net, params[name], device)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def get_params(name):\n",
    "    return params[name]\n",
    "\n",
    "def get_strategy(name):\n",
    "    if name == \"RandomSampling\":\n",
    "        return RandomSampling\n",
    "    elif name == \"LeastConfidence\":\n",
    "        return LeastConfidence\n",
    "    elif name == \"MarginSampling\":\n",
    "        return MarginSampling\n",
    "    elif name == \"EntropySampling\":\n",
    "        return EntropySampling\n",
    "    elif name == \"BALDDropout\":\n",
    "        return BALDDropout\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "# albl_list = [MarginSampling(X_tr, Y_tr, idxs_lb, net, handler, args),\n",
    "#              KMeansSampling(X_tr, Y_tr, idxs_lb, net, handler, args)]\n",
    "# strategy = ActiveLearningByLearning(X_tr, Y_tr, idxs_lb, net, handler, args, strategy_list=albl_list, delta=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--seed SEED] [--picknum PICKNUM]\n",
      "                             [--otherchoice OTHERCHOICE]\n",
      "                             [--n_init_labeled N_INIT_LABELED]\n",
      "                             [--n_query N_QUERY] [--n_round N_ROUND]\n",
      "                             [--dataset_name {MNIST,FashionMNIST,SVHN,CIFAR10,THEBE,FAULTSEG}]\n",
      "                             [--strategy_name {RandomSampling,MarginSampling,EntropySampling,BALDDropout}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\86177\\AppData\\Roaming\\jupyter\\runtime\\kernel-v39df8f5bd68f21b286f8e614d0fd0b4f4360d2acc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "# from utils import get_dataset, get_net, get_strategy\n",
    "from pprint import pprint\n",
    "# from common_tools import create_logger \n",
    "import os\n",
    "import random\n",
    "# from model_predict_thebe import model_predict\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=456, help=\"random seed\")    #111111111\n",
    "parser.add_argument('--picknum', type=int, default=50, help=\"random seed\")  \n",
    "parser.add_argument('--otherchoice', type=str, default=\"transunt_3\", help=\"number of round pick samples\")    #30pices\n",
    "parser.add_argument('--n_init_labeled', type=int, default=348, help=\"number of init labeled samples\")\n",
    "parser.add_argument('--n_query', type=int, default=50, help=\"number of queries per round\")\n",
    "parser.add_argument('--n_round', type=int, default=10, help=\"number of rounds\")\n",
    "parser.add_argument('--dataset_name', type=str, default=\"THEBE\", choices=[\"MNIST\", \"FashionMNIST\", \"SVHN\", \"CIFAR10\",\"THEBE\",\"FAULTSEG\"], help=\"dataset\")\n",
    "parser.add_argument('--strategy_name', type=str, default=\"EntropySampling\",\n",
    "                    choices=[\"RandomSampling\",                              \n",
    "                             \"MarginSampling\", \n",
    "                             \"EntropySampling\", \n",
    "                             \"BALDDropout\", ], help=\"query strategy\")\n",
    "args= parser.parse_args()\n",
    "pprint(vars(args))\n",
    "print()\n",
    " ################创建文件夹\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./active_learning_data/{}_{}\".format(args.seed,args.otherchoice)):\n",
    "\n",
    "    os.makedirs(\"./active_learning_data/{}_{}\".format(args.seed,args.otherchoice))\n",
    "\n",
    "os.makedirs(\"./active_learning_data/{}_{}/{}\".format(args.seed,args.otherchoice,args.strategy_name))\n",
    "os.makedirs(\"./active_learning_data/{}_{}/{}/log\".format(args.seed,args.otherchoice,args.strategy_name))\n",
    "os.makedirs(\"./active_learning_data/{}_{}/{}/predick_result\".format(args.seed,args.otherchoice,args.strategy_name))\n",
    "os.makedirs(\"./active_learning_data/{}_{}/{}/pick\".format(args.seed,args.otherchoice,args.strategy_name))\n",
    "os.makedirs(\"./active_learning_data/{}_{}/{}/picture\".format(args.seed,args.otherchoice,args.strategy_name))\n",
    "os.makedirs(\"./active_learning_data/{}_{}/{}/picture/test\".format(args.seed,args.otherchoice,args.strategy_name))\n",
    "os.makedirs(\"./active_learning_data/{}_{}/{}/picture/val\".format(args.seed,args.otherchoice,args.strategy_name))\n",
    "\n",
    "logger = create_logger(\"./active_learning_data/{}_{}/{}/log\".format(args.seed,args.otherchoice,args.strategy_name),\"main\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logger.info(args)\n",
    "\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(args.seed)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "dataset = get_dataset(args.dataset_name)                   # load dataset\n",
    "net = get_net(args.dataset_name, device)                   # load network\n",
    "strategy = get_strategy(args.strategy_name)(dataset, net)  # load strategy\n",
    "\n",
    "# start experiment\n",
    "# dataset.initialize_labels(args.n_init_labeled)\n",
    "print(f\"number of labeled pool: {args.n_init_labeled}\")\n",
    "# print(f\"number of unlabeled pool: {dataset.n_pool-args.n_init_labeled}\")\n",
    "print(f\"number of testing pool: {dataset.n_test}\")\n",
    "print()\n",
    "\n",
    "logger.info(f\"number of labeled pool: {args.n_init_labeled}\")\n",
    "# logger.info(f\"number of unlabeled pool: {dataset.n_pool-args.n_init_labeled}\")\n",
    "logger.info(f\"number of testing pool: {dataset.n_test}\")\n",
    "\n",
    "\n",
    "\n",
    "best_iou=strategy.train_before(0,args.strategy_name,args.seed,args.otherchoice)  \n",
    "# flag=np.ones([15,512,2048],type=\"bool\")\n",
    "flag=np.ones([15])\n",
    "\n",
    "\n",
    "# ############################################################\n",
    "\n",
    "for rd in range(1, args.n_round+1):\n",
    "\n",
    "    print(f\"Round {rd}\")\n",
    "    logger.info(f\"Round {rd}\")\n",
    "\n",
    "    # query\n",
    "    flag_update = strategy.query(rd,args.seed,args.otherchoice,args.picknum,args.picknum_no,flag)#n_query 10 \n",
    "    flag=flag_update \n",
    "    # strategy.update(query_idxs)\n",
    "    a=strategy.train(rd,args.strategy_name,best_iou,args.seed,args.otherchoice)\n",
    "   \n",
    "    best_iou=a\n",
    "    print(best_iou)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55bdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
